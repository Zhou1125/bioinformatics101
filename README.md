# bioinformatics101


New long-reads mapper Vulcan



[Vulcan: Improved long-read mapping and structural variant calling via dual-mode alignment](https://academic.oup.com/gigascience/article/10/9/giab063/6375129)






Alan E. Yocca [github](https://github.com/Aeyocca) | [ka_ks](https://github.com/Aeyocca/ka_ks_pipe) | [MCScanX](https://github.com/Aeyocca/MCScanX) | [Core_Prediction Pangenome](https://github.com/Aeyocca/Core_Prediction)






[Atria: an ultra-fast and accurate trimmer for adapter and quality trimming](https://gigabytejournal.com/articles/31)


![img](https://gigabytejournal.com/admin/apis/public/article_shell/uploads/article_files/Gigabyte/Gigabyte/2021/gigabyte31/gigabyte31-g001.jpg)



[Webinar Recording Now Available – Bioinformatics Workflows for High Quality Genome Assembly in Plants and Animals](https://github.com/Yedomon/bioinformatics101/tree/main/Pacbio_Genome_Workflow)



[Genome-Wide Analysis of Terpene Synthase Gene Family in Mentha longifolia and Catalytic Activity Analysis of a Single Terpene Synthase](https://www.mdpi.com/2073-4425/12/4/518)


> The gene structure of TPSs from M. longifolia was determined based on annotation information and then illustrated using Exon-Intron Graphic Maker (http://www.wormweb.org/exonintron). Subcellular localization of M. longifolia TPSs was predicted using the AtSubP tool (http://bioinfo3.noble.org/AtSubP/index.php) and ProtComp (http://linux1.softberry.com/berry.phtml?topic=protcomppl&group=programs&subgroup=proloc). The location of M. longifolia TPS genes on the scaffold was determined by Tbtools [35]. Tandemly duplicated genes were identified by their sequence similarity and scaffold localization according to earlier studies [36,37]. The conserved motifs of M. longifolia TPSs, including the RR(X)8W motif, NSE/DTE motif, RXR motif, and DDXXD motif, were identified based on the multiple sequence alignment results.










[Reproducible, scalable, and shareable analysis pipelines with bioinformatics workflow managers](https://www.nature.com/articles/s41592-021-01254-9.epdf?sharing_token=PVE8zdv0ymug_6OM4_QgKNRgN0jAjWel9jnR3ZoTv0OsA3GwSLYb6fNLyGpuUMTaYvo9Wdf90KP2BE13GkdM7BlFsIy03fWrZixiMpUSRJoIU1kqWIrPWKqMeI_kRIqWJMXbKlin7ZVPx4oXRGuohgq3SXFo76LILVPqk3Z1n3E%3D)


[github](https://github.com/GoekeLab/bioinformatics-workflows)



[Another new tutorial posted in the bioinformatics workbook: Eliminating contaminating reads to create a more contiguous genome assembly. Get that draft assembly nice, because nobody wants to scaffold a million little contigs.](https://bioinformaticsworkbook.org/dataAnalysis/GenomeAssembly/IteratingGenomeAssemblyWithReadFiltration.html#gsc.tab=0) 




## ParaAT

```python

/usr/bin/perl /NABIC/HOME/soyounwon/bin//ParaAT.pl -h RLK.pair.txt -n cds.renamed.fa -a Rgene.pep.renamed.fa -m muscle -g -kaks -f axt -p proc -o RLK
KaKs_Calculator -i CWT00062306-CWT00062399.cds_aln.axt -o CWT00062306-CWT00062399.cds_aln.axt.kaks

```




[WGCNA Gene Correlation Network Analysis](https://bioinformaticsworkbook.org/dataAnalysis/RNA-Seq/RNA-SeqIntro/wgcna.html#gsc.tab=0)


[blockwise module](https://alexslemonade.github.io/refinebio-examples/04-advanced-topics/network-analysis_rnaseq_01_wgcna.html#47_Run_WGCNA)

[amazon](https://rstudio-pubs-static.s3.amazonaws.com/687551_ed469310d8ea4652991a2e850b0018de.html)



[GENAVi: a shiny web application for gene expression normalization, analysis and visualization](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6073-7)

![img](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12864-019-6073-7/MediaObjects/12864_2019_6073_Fig1_HTML.png?as=webp)



[Transposable element diversity remains high in gigantic genomes](https://www.biorxiv.org/content/10.1101/2021.08.27.457980v1.full.pdf+html)

[Comprehensive analysis of structural variants in breast cancer genomes using single-molecule sequencing](https://genome.cshlp.org/content/30/9/1258.full#sec-8)






[bcftools merge issue](https://github.com/samtools/bcftools/issues/668)








[picard remove duplication](https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard-)


[picard remove duplication in Korean](http://www.incodom.kr/PICARD/MarkDuplicates)


[Fifteen quick tips for success with HPC, i.e., responsibly BASHing that Linux cluster](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009207)

great summary of Young investisor SMRT very helpfull


[summary](https://www.pacb.com/blog/young-investigators-share-stellar-science-career-advice-and-bioinformatics-tools-at-smrt-leiden-2021/)


Key notes
> Hubert Pausch of ETH Zürich discussed the downfalls of reference-guided variant discovery and the benefits of genome graphs to overcome some of the biases of linear mapping.

So reference-based assembly also not so good.... need to make a pangenome graph


> “PacBio long-read sequencing is ushering in a new era in human genomics.” — Tychele Turner, PhD., Washington University Genetics


> Choose your mentor over your scientific subject. Learning the craft and being the best scientist you can be is most important, and can then be applied to the subject you love. Also, don’t stymy your creativity. Practice it. Try to have one new idea every day—it doesn’t even matter if it involves science. — Jonas Korlach, PacBio CSO


> Drive yourself. Don’t compare yourself to anyone else. Don’t rely on anyone else to set expectations and hold yourself accountable. Remove the word ‘should’ from your career vocabulary. — Melissa Smith

> Be open to trying something new. And don’t be cowed by others’ successes. They’ve encountered challenges too. — Tychele Turner, PhD., Washington University Genetics
> On being open to revision and excited by the chance to iterate:

Be happy if you see your papers returned covered in red (edits) – it means your supervisor cares. Don’t be frustrated. Take their advice and use it to improve. — Hubert Pausch, Prof. Dr., ETH Zurich

And, last but not least – on passion:

You can still make great contributions to science outside of academia. Even extracurricular activities like roller derby can teach you key soft skills like leadership and organization. Pursue all your passions. — Sarah Kingan, Senior Product Manager at PacBio

And, don’t forget to leave the lab every once in a while.


2021 -  [OVarFlow: a resource optimized GATK 4 based Open source Variant calling workFlow](https://www.biorxiv.org/content/10.1101/2021.05.12.443585v1.full)


[gitlab](https://gitlab.com/computational-biology/ovarflow)  | [docker](https://hub.docker.com/r/ovarflow/release/tags?page=1&ordering=last_updated) | [singularity](https://zenodo.org/record/4746639#.YQy7Tz_iuUk)


[snp calling gatk 1](https://qcb.ucla.edu/wp-content/uploads/sites/14/2016/03/GATK_Discovery_Tutorial-Worksheet-AUS2016.pdf)


[snp calling gatk 2](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4243306/pdf/nihms531590.pdf)


## looping using lines of a file



See [here](https://codefather.tech/blog/bash-loop-through-lines-file/)


```python


FILENAME="european-cities.txt" # step 1


LINES=$(cat $FILENAME) # step 2


for LINE in $LINES
do
    echo "$LINE"
done # step3


```






[Benchmarking code](https://github.com/genomeassembler/benchmarking-study)


Need Help?

If you want to get in contact about something one-on-one, please contact Dean at dean.c.southwood@gmail.com.















corvid SV popgen


---
title: "Custom scripts and pipelines"
subtitle: "The population genomics of structural variation in a songbird genus"
author: "Matthias H. Weissensteiner"
date: "January 8, 2020"
output:
  rmarkdown::github_document:
    toc: yes
---

# Assembly-based SV detection

## SV detection with `MUMMer` and `Assemblytics`

First align the associated contigs of the `FALCON UNZIP` assembly to the primary assembly:
```{bash, eval = FALSE}
MUMmer3.23/nucmer -maxmatch -l 100 -c 500 \
   primary_assembly.fas associated_contigs.fas \
   -prefix associated_vs_primary_assembly
```

The resulting delta file was then uploaded to the `Assemblytics` webpage (www.assemblytics.com) where the genome comparison 
was run at default parameters. The Assemblytics output was filtered and converted to `vcf` using `SURVIVOR`:

```{bash, eval = FALSE}
SURVIVOR convertAssemblytics assemblytics_output 0 temp

cat temp | tr ";" "\t" | sed 's/SVLEN=//g' | awk '$13<2000' | \
 awk '{print $1 "\t" $2 "\t" $3 "\t"$4 "\t" $5 "\t" $6 "\t" $7 "\t" $8 ";" $9 ";" 
 $10 ";" $11 ";" $12 ";SVLEN=" $13 ";" $14 "\t" $15}' > assemblytics_output.vcf
```

## SV detection with `smartie-sv`

We ran `smartie-sv` with the default Snakefile using primary and associated assemblies as input. The output
was filtered and converted to vcf with `SURVIVOR`:

```{bash, eval = FALSE}
snakemake -p -w 25 -j 3 \
  --verbose \
  -s Snakefile \
  --cluster-config cluster.config.slurm.json \
  --cluster "sbatch -A {cluster.partition} -n {cluster.n}  -t {cluster.time} -c {cluster.c} \
  -o out -e error" 
  -w 30 
awk '{print $1,$2,$3,$1,$2+$5,$3+$5,$4,$4,$13,$6,$14}' smartie-sv_output  | \
sed 's/ /\t/g' | sed -e 's/insertion/INS/g' -e 's/deletion/DEL/g'  > temp
SURVIVOR bedpetovcf smartie-sv_output.vcf
```

## Merge `Assemblytics` and `smartie-sv` output

This was done using `SURVIVOR`:

```{bash, eval = FALSE}
SURVIVOR merge \
  1000 \
  1 \
  1 \
  0 \
  0 \
  30 \
  associated_contigs_vs_primary_assembly_Assemblytics_smartiesv.vcf

grep -v '#' associated_contigs_vs_primary_assembly_Assemblytics_smartiesv.vcf | \
 tr ";" "\t" | cut -f 1,2,5,10 | sed -e 's/AVGLEN=//g' -e 's/<//g' -e 's/>//g' > \
 associated_contigs_vs_primary_assembly_Assemblytics_smartiesv.table
```

## Assembly-based SV visualization and density per 1 Mb window

```{R, eval = FALSE}
#loading the data
dat_JD<-read.table("associated_contigs_vs_primary_assembly_Assemblytics_smartiesv.table", header=F)
colnames(dat_JD)<-c("chr", "pos", "type", "len")
win<-read.table("1Mb_windows.genome.bed", header=F)
colnames(win)<-c("chr", "start", "end")
chr_lengths<-read.table("genome_file.txt", header=T)

# Filter for full 1 Mb windows:

win %>% mutate(win_length=end-start) %>% 
  filter(win_length == 1000000) -> filtered_win
count<-c()
average_len<-c()
for (i in 1:length(filtered_win$chr)) {
  dat_JD %>% 
    filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>% 
    select(len) %>%
    unlist() %>%
    as.vector() %>%
    length() -> count[i]
  dat_JD %>% 
    filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>%  
    select(len) %>%
    unlist() %>%
    as.vector() %>%
    mean() -> average_len[i]
}

filtered_win %>% mutate(count, average_len) -> filtered_win

xlabel<-"Count"
ylabel<-"Frequency"
title<-"SV count per 1Mb window"
filtered_win %>% 
  ggplot(aes(count)) +
  geom_histogram(binwidth = 1,colour="#000000", fill="#bdbdbd", aes(y=..density..), size=0.2) + 
  xlim(-2,25) +
  theme_classic(base_size = 9) +
  xlab(xlabel)+ 
  ylab(ylabel)+
  theme(
        strip.text = element_text(),
        plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        legend.position="none",
        axis.title.x=element_text(size=12),
        axis.title.y=element_blank(),
        axis.text.x=element_text(size=9),
        axis.text.y=element_text(size=9)) 

# Set different size categories:

dat_JD<-read.table("primary_contigs_JDv2.3-associated_contigs_JDv2.0.0_Assemblytics_smartiesv.SURVIVOR.table", header=F)
colnames(dat_JD)<-c("chr", "pos", "type", "len")
win<-read.table("1Mb_windows.genome.JDv2.3.bed", header=F)
colnames(win)<-c("chr", "start", "end")
chr_lengths<-read.table("JDv2.3.genome_file.txt", header=T)


chr_lengths<-chr_lengths[order(-chr_lengths$length),]
chr_lengths
include_chr<-chr_lengths$ID[chr_lengths$length>1000000]
include_chr
filtered_dat_JD<-dat_JD[dat_JD$chr %in% include_chr,]
filtered_dat_JD$chr<-factor(filtered_dat_JD$chr, levels=include_chr)


# Filter for full 1 Mb windows:

win %>% mutate(win_length=end-start) %>% 
  filter(win_length == 1000000) -> filtered_win

head(filtered_dat_JD)

filtered_dat_JD %>% filter(len >50 & len < 500) -> dat_JD_50_500
filtered_dat_JD %>% filter(len >500 & len < 1000) -> dat_JD_500_1000
filtered_dat_JD %>% filter(len >1000 & len < 5000) -> dat_JD_1000_5000
filtered_dat_JD %>% filter(len >5000) -> dat_JD_larger_than_5000

dat_JD <- dat_JD_50_500
count<-c()
average_len<-c()
for (i in 1:length(filtered_win$chr)) {
  dat_JD %>% filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>% 
    select(len) %>%
    unlist() %>%
    as.vector() %>%
    length() -> count[i]
  dat_JD %>% filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>%  
    select(len) %>%
    unlist() %>%
    as.vector() %>%
    mean() -> average_len[i]
}

filtered_win %>% mutate(count, average_len) -> filtered_win_JD

xlabel<-"Count"
ylabel<-"Frequency"
title<-"Jackdaw"
#title<-"jackdaw SV count per 1Mb window"
filtered_win_JD %>% filter(count > 0) %>%
  ggplot(aes(count)) +
  geom_histogram(binwidth = 1,colour="#000000", fill="#bdbdbd",  size=0.2) + 
  xlim(-2,25) + ylim(0,260) +
  theme_classic(base_size = 9) +
  ylab(ylabel)+
  ggtitle(title)+
  theme(
    strip.text = element_text(),
    plot.margin = unit(c(5,5,5,5),"mm"),
    panel.spacing.x = unit(2,"mm"),
    legend.position="none",
    axis.title.x=element_blank(),
    axis.title.y=element_blank(),
    plot.title = element_text(size = 12, hjust=0.5),
    axis.text.x=element_text(size=9),
    axis.text.y=element_text(size=9)) -> JD_50_500

dat_JD <- dat_JD_500_1000
count<-c()
average_len<-c()
for (i in 1:length(filtered_win$chr)) {
  dat_JD %>% filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>% 
    select(len) %>%
    unlist() %>%
    as.vector() %>%
    length() -> count[i]
  dat_JD %>% filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>%  
    select(len) %>%
    unlist() %>%
    as.vector() %>%
    mean() -> average_len[i]
}

filtered_win %>% mutate(count, average_len) -> filtered_win_JD

xlabel<-"Count"
ylabel<-"Frequency"
#title<-"jackdaw SV count per 1Mb window"
filtered_win_JD %>% filter(count > 0) %>%
  ggplot(aes(count)) +
  geom_histogram(binwidth = 1,colour="#000000", fill="#bdbdbd",  size=0.2) + 
  xlim(-2,25) + ylim(0,260) +
  theme_classic(base_size = 9) +
  ylab(ylabel)+
  theme(
    strip.text = element_text(),
    plot.margin = unit(c(5,5,5,5),"mm"),
    panel.spacing.x = unit(2,"mm"),
    legend.position="none",
    axis.title.x=element_blank(),
    axis.title.y=element_blank(),
    axis.text.x=element_text(size=9),
    axis.text.y=element_text(size=9)) -> JD_500_1000

dat_JD <- dat_JD_100_5000
count<-c()
average_len<-c()
for (i in 1:length(filtered_win$chr)) {
  dat_JD %>% filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>% 
    select(len) %>%
    unlist() %>%
    as.vector() %>%
    length() -> count[i]
  dat_JD %>% filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>%  
    select(len) %>%
    unlist() %>%
    as.vector() %>%
    mean() -> average_len[i]
}

filtered_win %>% mutate(count, average_len) -> filtered_win_JD

xlabel<-"Count"
ylabel<-"Frequency"
#title<-"jackdaw SV count per 1Mb window"
filtered_win_JD %>% filter(count > 0) %>%
  ggplot(aes(count)) +
  geom_histogram(binwidth = 1,colour="#000000", fill="#bdbdbd",  size=0.2) + 
  xlim(-2,25) + ylim(0,260) +
  theme_classic(base_size = 9) +
  ylab(ylabel)+
  theme(
    strip.text = element_text(),
    plot.margin = unit(c(5,5,5,5),"mm"),
    panel.spacing.x = unit(2,"mm"),
    legend.position="none",
    axis.title.x=element_blank(),
    axis.title.y=element_blank(),
    axis.text.x=element_text(size=9),
    axis.text.y=element_text(size=9)) -> JD_1000_5000

dat_JD <- dat_JD_larger_than_5000
count<-c()
average_len<-c()
for (i in 1:length(filtered_win$chr)) {
  dat_JD %>% filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>% 
    select(len) %>%
    unlist() %>%
    as.vector() %>%
    length() -> count[i]
  dat_JD %>% filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>%  
    select(len) %>%
    unlist() %>%
    as.vector() %>%
    mean() -> average_len[i]
}

filtered_win %>% mutate(count, average_len) -> filtered_win_JD

xlabel<-"Count"
ylabel<-"Frequency"
#title<-"jackdaw SV count per 1Mb window"
filtered_win_JD %>% filter(count > 0) %>%
  ggplot(aes(count)) +
  geom_histogram(binwidth = 1,colour="#000000", fill="#bdbdbd",  size=0.2) + 
  xlim(-2,25) + ylim(0,260) +
  theme_classic(base_size = 9) +
  xlab(xlabel)+ 
  ylab(ylabel)+
  theme(
    strip.text = element_text(),
    plot.margin = unit(c(5,5,5,5),"mm"),
    panel.spacing.x = unit(2,"mm"),
    legend.position="none",
    axis.title.x=element_text(size=12),
    axis.title.y=element_blank(),
    axis.text.x=element_text(size=9),
    axis.text.y=element_text(size=9)) -> JD_larger_than_5000

```

## Comparing size distributions of assembly-based SVs:

``` {R, eval = FALSE}
dat_HC<-read.table("primary_contigs_HCv5.5-associated_contigs_HCv5.0_Assemblytics_smartiesv.SURVIVOR.table", header=F)
colnames(dat_HC)<-c("chr", "pos", "type", "len")
dat_JD<-read.table("primary_contigs_JDv2.3-associated_contigs_JDv2.0.0_Assemblytics_smartiesv.SURVIVOR.table", header=F)
colnames(dat_JD)<-c("chr", "pos", "type", "len")
dat_HAC<-read.table("primary_contigs_HAC-associated_contigs_HAC.0_Assemblytics_smartiesv.SURVIVOR.table", header=F)
colnames(dat_HAC)<-c("chr", "pos", "type", "len")


dat_HC %>%
  ggplot(aes(len)) + 
  geom_histogram(binwidth = 10, aes(y=..density..)  ) +
  theme_classic(base_size = 9) +
  ylab("Hooded crow") + 
  xlim(0,10000)-> len_hist_HC

dat_JD %>%
  ggplot(aes(len)) + 
  geom_histogram(binwidth = 10, aes(y=..density..)) +
  theme_classic(base_size = 9) +
  ylab("Jackdaw") +
  xlim(0,10000)-> len_hist_JD

dat_HAC %>%
  ggplot(aes(len)) + 
  geom_histogram(binwidth = 10, aes(y=..density..)) +
  theme_classic(base_size = 9) +
  ylab("Hawaiian crow") + 
  xlim(0,10000)-> len_hist_HAC

pdf("Assembly_based_SV_length_distribution.pdf", width=7, height=4)
grid.arrange(len_hist_HC, len_hist_JD, len_hist_HAC, nrow=3)
dev.off()
```
# Assembly-based SNP detection

This was done using the `show-snps` function of `MUMmer`, following alignment of the primary and associated assemblies:

```{bash, eval = FALSE}
MUMmer3.23/nucmer -maxmatch -l 100 -c 100 \
primary_assembly.fas associated_contigs.fas -prefix associated_vs_primary_assembly
delta-filter -r -q associated_vs_primary_assembly > associated_vs_primary_assembly.filter
show-snps -Clr -T associated_vs_primary_assembly.filter > \
associated_vs_primary_assembly.filter.snps
```

Then the visualization and SNP density calculation was done in `R`:
```{R, eval = FALSE}
# Load data
dat<-read.table("associated_vs_primary_assembly.filter.snps", header=F)
colnames(dat)<-c("chr", "pos", "end")
win<-read.table("1Mb_windows.genome.bed", header=F)
colnames(win)<-c("chr", "start", "end")
chr_lengths<-read.table("genome_file.txt", header=T)

# Filter for full 1 Mb windows:

win %>% mutate(win_length=end-start) %>% 
  filter(win_length == 1000000) -> filtered_win

count<-c()
for (i in 1:length(filtered_win$chr)) {
print(filtered_win[i,1])
 dat %>% 
  filter(chr == paste(filtered_win[i,1]) & pos >= filtered_win[i,2] & pos <= filtered_win[i,3]) %>%
  dim() -> x
  count[i]<-x[1]
}
filtered_win %>% mutate(count) -> filtered_win

xlabel<-"count per 1Mb window"
ylabel<-"Frequency"
title<-"SNPs per 1Mb window"
filtered_win %>% 
  ggplot(aes(count)) +
  geom_histogram(binwidth = 100,colour="#000000", fill="#bdbdbd") + 
  theme_classic(base_size = 12) +
  xlab(xlabel)+ 
  ylab(ylabel)+
ggtitle(title) + ylim(-1,450) + xlim(-100,3500) +
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(hjust = 0.5),
        legend.position="none",
        axis.title.x=element_text(),
        axis.text.x=element_text(),
        plot.title= element_text(hjust= 0.5),
        axis.title.y=element_text(),
        strip.text.x = element_text(angle=90, margin=unit(c(15,5,15,5),"mm"), size=15), 
        strip.background = element_blank())

```

# Read-mapping based SV detection

## Long-read mapping based SV Detection

As a first step, (long) reads are aligned to a reference assembly using NGM-LR.
Resulting bam files are sorted and indexed:

```{bash, eval = FALSE}
ngmlr \
   -r reference \
   -q individual.fastq \
   -t 16 \
   -x pacbio \
   | samtools view -Sb - >  individual.bam

samtools sort individual.bam  -o individual.sorted.bam
samtools index individual.sorted.bam
```

Next, these bam files are used to call SVs with `Sniffles`:

```{bash, eval = FALSE}
sniffles \
   -m individual.sorted.bam \
   -v individual.vcf \
   -s 5 \
   --threads 1 \
   --genotype \
   --cluster
```

The `-s 5` option specifies the minimum number of reads supporting a given variant.

Next, the individual vcf files were filtered using `bcftools` and `grep` according to the following criteria:
allele frequency (based on supporting reads) > 0.3
SV length < 100 kb (to remove erroneous chromosome-scale variants)
Number of supporting reads < 60 (to remove variants caused by high-copy repeats)
Remove translocations (again to remove numerous erroneous variants)

```{bash, eval = FALSE}
grep -v '<TRA>' individual.vcf | bcftools view -i 'AF>0.3 && SVLEN<100000 && RE[1]<60'  -  > \
individual_filtered.vcf 
```

These individual files were then merged using `SURVIVOR` 
```{bash, eval = FALSE}
SURVIVOR \
   merge \
   vcf_list_with_all_single_vcf_files \
   1000 \
   1 \
   1 \
   0 \
   0 \
   50 \
   merged_filtered.vcf
```
Here the filtering parameters were the following: 
max distance between breakpoints: 1000
Minimum number of supporting caller: 1
Take the type into account (1==yes, else no): 1
Take the strands of SVs into account (1==yes, else no): 0
Estimate distance based on the size of SV (1==yes, else no): 0
Minimum size of SVs to be taken into account: 50 

This merged vcf file built from all single vcf files was then used as the basis for another
round of SV calling, this time with the `--Ivcf` option which enables the input of a list of
SVs which are then genotyped given the bam files. Additionally, we used the `--min_homo_af 0.7` option
to improve genotype accuracy (after some initial testing). 
```{bash, eval = FALSE}
sniffles \
   -m individual.sorted.bam \
   -v individual_force_called.vcf \
   --not_report_seq \
   --min_homo_af 0.7 \
   --threads 1 \
   --genotype \
   --Ivcf merged_filtered.vcf
```
The individual force-called vcfs were then again merged with `Sniffles`:
```{bash, eval = FALSE}
SURVIVOR \
   merge \
   vcf_list_with_all_single_force_called_vcf_files \
   1000 \
   1 \
   1 \
   0 \
   0 \
   50 \
   merged_filtered_force_called.vcf
```

Before these variants with the corresponding genotypes per individuals were used for further analysis,
we performed further filtering steps. First we removed variants overlapping with gaps (as denoted by 'Ns' in the reference sequence):

```{bash, eval = FALSE}
vcf = merged_filtered_force_called.vcf
bed = Ns_HCv5.5.bed
bedtools intersect -v -a $vcf -b $bed | cat <(grep '#' $vcf) - > temp
```

We then converted this vcf file to a genotype file with `vcftools` for downstream analysis in `R`:
The `pop` file is a single-column file with numbers for respective populations / species. 

```{bash, eval = FALSE}
vcf_file = temp
output_1 = merged_filtered_force_called.GT_format
output_2 = merged_filtered_force_called.chr_pos_type_len

vcftools --vcf ${vcf_file} --extract-FORMAT-info GT --stdout |\ 
 tail -n +2 | \ 
 cut -f 3- | cat -n | \  
 python -c \
 "import sys; print('\n'.join(' '.join(c) for c in zip(*(l.split() for l in sys.stdin.readlines() \
 if l.strip()))))" - | \ 
 sed -e 's/0\/0/0/g' -e 's/0\/1/1/g' -e 's/1\/1/2/g' -e 's/\.\/\./NA/g'  | \ 
 paste pop -  | tr "\t" " " > $output_1  
 
echo 'chr       pos     type    len' >> $output_2
grep -v '#' $vcf_file | tr ";" "\t" |\ 
  cut -f 1,2,5,10 | \    
   sed -e 's/AVGLEN=//g' -e 's/>//g' -e 's/<//g' \
   -e 's/SVLEN=-//g' -e 's/SVLEN=//g' -e 's/\//_/g'  >> $output_2
   
```

## Downstream analysis of LR variants in `R`

The genotype file was then used in the downstream analysis: 


First load the used packages:
```{R, eval = FALSE}

library(easypackages)
libraries("ggplot2", "plyr", "tidyr", "dplyr", "purrr", "grid",
          "gridExtra", "lme4", "SNPRelate", "gdsfmt")
`%not_in%` <- purrr::negate(`%in%`)
```

Then load the genotype, variant and chromosome length file:
```{R, eval = FALSE}
dat<-read.table("merged_filtered_force_called.GT_format", header=T) 
loc<-read.table("merged_filtered_force_called.chr_pos_type_len", header=T)
chr_lengths<-read.table("v5.5.genome_file.txt", header=T)
colnames(chr_lengths)<-c("id", "length")  
```

### PCA

Next, perform the PCA with the LR genotypes:
```{R, eval = FALSE}
backup<-dat
genos<-dat[,2:length(dat[1,])]
samples <- read.table("data_pops", header=FALSE)
names(samples)<-c("pop", "pop_nr", "sampleID")

closefn.gds(SVs) 
genotypes <- as.matrix(genos)
sample.ids <- as.character(samples$sampleID)
SV.ids <- paste("SV",1:ncol(genotypes),sep="")

snpgdsCreateGeno("SVs.gds", genmat=genotypes, sample.id=sample.ids,
                 snp.id=SV.ids, snpfirstdim=FALSE)
SVs <- openfn.gds("SVs.gds", readonly=FALSE)

pca <- snpgdsPCA(SVs, snp.id=SV.ids, num.thread=7, sample.id=sample.ids)
pca_perc<-pca$varprop*100

pop <- c("HC_genome_ind", "HC", "HC", "HC", "HC", "HC", "HC", "HC", "HC", 
         "CC", "CC", "CC", "CC", "CC", "CC", "CC", "CC", "CCE", "CCE", "CCE", "CCE", "CCE", "CCE",
         "AC", "AC", "JD", "JD" , "JD", "JD", "JD", "DJD", "DJD", "DJD")
pca$eigenvect %>% data.frame() %>% select(X1, X2, X3, X4, X5, X6) -> pca_eigenvectors
names(pca_eigenvectors) <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6")
pca_eigenvectors <- cbind(pop, pca_eigenvectors)
  
col<-c("red", "blue", "green", "orange", "black", "brown")
values<-c("DJD", "AC", "CCE", "JD", "CC", "HC")
names(col)<- values
pca_eigenvectors %>% filter(pop != "HC_genome_ind") %>% 
     ggplot(aes(PC1, PC2, colour=pop)) +
     geom_point(size = 2)  + scale_colour_manual(values=col, guide= FALSE) +
     theme_classic(base_size = 9) + 
     xlab(paste0("PC 1( ", round(pca_perc[1], digits=3), " %)")) + 
     ylab(paste0("PC 2( ", round(pca_perc[2], digits=3), " %)")) -> pca_all
```

Now repeat PCA for the respective clades (crow and jackdaw) and for the European crow populations and plot all in one figure:

```{R, eval = FALSE}
dat<-backup
genos_crow_clade<-dat[1:25,2:length(dat[1,])]
samples <- read.table("data_pops", header=FALSE)
names(samples)<-c("pop", "pop_nr", "sampleID")
samples_crow_clade<-samples[1:25,]

closefn.gds(SVs) 
genotypes <- as.matrix(genos_crow_clade)
sample.ids <- as.character(samples_crow_clade$sampleID)
SV.ids <- paste("SV",1:ncol(genotypes),sep="")

snpgdsCreateGeno("SVs.gds", genmat=genotypes, sample.id=sample.ids, snp.id=SV.ids, snpfirstdim=FALSE)
SVs <- openfn.gds("SVs.gds", readonly=FALSE)

pca <- snpgdsPCA(SVs, snp.id=SV.ids, num.thread=7, sample.id=sample.ids)
pca_perc<-pca$varprop*100

pop <- c("HC_genome_ind", "HC", "HC", "HC", "HC", "HC", "HC", "HC", "HC", 
         "CC", "CC", "CC", "CC", "CC", "CC", "CC", "CC", "CCE", "CCE", "CCE", "CCE", "CCE", "CCE",
         "AC", "AC", "JD", "JD" , "JD", "JD", "JD", "DJD", "DJD", "DJD")
pop<-pop[1:25]
pca$eigenvect %>% data.frame() %>% select(X1, X2, X3, X4, X5, X6) -> pca_eigenvectors
names(pca_eigenvectors) <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6")
pca_eigenvectors <- cbind(pop, pca_eigenvectors)

col<-c("red", "blue", "green", "orange", "black", "brown")
values<-c("DJD", "AC", "CCE", "JD", "CC", "HC")
names(col)<- values
pca_eigenvectors %>% filter(pop != "HC_genome_ind") %>% 
  ggplot(aes(PC1, PC2, colour=pop)) +
  geom_point(size = 2)  + scale_colour_manual(values=col, guide= FALSE) +
    theme_classic(base_size = 9) +
    xlab(paste0("PC 1( ", round(pca_perc[1], digits=3), " %)")) + 
    ylab(paste0("PC 2( ", round(pca_perc[2], digits=3), " %)")) -> pca_crow_clade
  
dat<-backup
genos_euro_crow_clade<-dat[1:23,2:length(dat[1,])]
samples <- read.table("data_pops", header=FALSE)
names(samples)<-c("pop", "pop_nr", "sampleID")
samples_euro_crow_clade<-samples[1:23,]

closefn.gds(SVs) 
genotypes <- as.matrix(genos_euro_crow_clade)
sample.ids <- as.character(samples_euro_crow_clade$sampleID)
SV.ids <- paste("SV",1:ncol(genotypes),sep="")

snpgdsCreateGeno("SVs.gds", genmat=genotypes, sample.id=sample.ids, snp.id=SV.ids, snpfirstdim=FALSE)
SVs <- openfn.gds("SVs.gds", readonly=FALSE)

pca <- snpgdsPCA(SVs, snp.id=SV.ids, num.thread=7, sample.id=sample.ids)
pca_perc<-pca$varprop*100

pop <- c("HC_genome_ind", "HC", "HC", "HC", "HC", "HC", "HC", "HC", "HC", 
         "CC", "CC", "CC", "CC", "CC", "CC", "CC", "CC", "CCE", "CCE", "CCE", "CCE", "CCE", "CCE", 
         "AC", "AC", "JD", "JD" , "JD", "JD", "JD", "DJD", "DJD", "DJD")
pop<-pop[1:23]
pca$eigenvect %>% data.frame() %>% select(X1, X2, X3, X4, X5, X6) -> pca_eigenvectors
names(pca_eigenvectors) <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6")
pca_eigenvectors <- cbind(pop, pca_eigenvectors)

col<-c("red", "blue", "green", "orange", "black", "brown")
values<-c("DJD", "AC", "CCE", "JD", "CC", "HC")
names(col)<- values
pca_eigenvectors %>% filter(pop != "HC_genome_ind") %>% 
    ggplot(aes(PC1, PC2, colour=pop)) + geom_point(size = 2)  + 
    scale_colour_manual(values=col, guide= FALSE) +
    theme_classic(base_size = 9) +
    xlab(paste0("PC 1( ", round(pca_perc[1], digits=3), " %)")) + 
    ylab(paste0("PC 2( ", round(pca_perc[2], digits=3), " %)")) -> pca_euro_crow_clade
  
svg("pca_LR.svg", height=2, width=7)
grid.arrange(pca_all, pca_crow_clade, pca_euro_crow_clade, ncol=3)
dev.off()
```

### 'Phylogenetic filtering' 

Next, we performed the 'phylogenetically-informed' filtering. 

```{R, eval = FALSE}
# Transpose the genotype table to get one individual per line and one variant per column
t_dat <- t(dat[,2:ncol(dat)]) 
t_dat %>% group_by(chr,pos, type, len) %>% 
  gather(individual, genotype, 9:41) %>% 
  data.frame() -> table
table$individual<-factor(table$individual, levels=c(1:33))   

# set individuals of species and populations
# hooded crow
hc<-c(2:9)
# German carrion crow
cc<-c(10:17)
# Spanish carrion crow
cce<-c(18:23)
# American crow
ac<-c(24:25)
# jackdaw
jd<-c(26:30)
# Daurian jackdaw

djd<-c(31:33)
all<-c(1:33)
crows<-c(2:25)
jd_clade<-c(26:33)

colors<-c("#33a02c", "#b2df8a", "#1f78b4", "#a6cee3")
n<-c(0,1,2, "genotype_NA")
names(colors)<-n

# use the chr_lengths file to sort chromosomes by size
chr_lengths<-chr_lengths[order(-chr_lengths$length),]
  chr_lengths
  include_chr<-chr_lengths$id[chr_lengths$length>0] # this is arbitrary and may be changed
  include_chr
  filtered_table<-table[table$chr %in% include_chr,]
  filtered_table$chr<-factor(filtered_table$chr, levels=include_chr)
  head(filtered_table)
  
# Introduce a unique identifier based on chromosome and position for each variant:
filtered_table %>% 
  mutate(SV_ID=paste0(chr,"_",pos)) %>% 
  mutate(new_genotype = genotype) -> temp
filtered_table <- temp

# Add a column to assess which variants have genotype `NA`
filtered_table$new_genotype[which(is.na(filtered_table$new_genotype))] <- "genotype_NA"  

# Exclude the genome individual 
  
filtered_table %>% filter(individual !=1) -> temp
filtered_table <- temp
 
# Sites which have two variants at the same position starting
filtered_table %>% filter(SV_ID %not_in% exclude_variants) -> temp
filtered_table <- temp

# Filter duplicated sites:

filtered_table %>% filter(individual == 2) %>% select(SV_ID) %>%
  mutate(dup = duplicated(SV_ID)) %>% filter(dup == "TRUE") %>% select(SV_ID) %>%
  unlist() %>% as.vector() -> duplicated_variants

filtered_table %>% filter(SV_ID %not_in% duplicated_variants) -> temp
filtered_table <- temp 

# Exclude variants which are not insertions, deletions or inversions: 
filtered_table %>% filter(type != "DUP") %>%
  filter(type != "DUP_INS") %>%
  filter(type != "INV_INVDUP") %>%
  filter(type != "INVDUP")-> temp
filtered_table <- temp

# Now look which variants are all homozygous variant or reference
# in the jackdaw clade, allowing for 2 errors:  
filtered_table %>% 
  filter(individual %in% jd_clade) %>%  
  group_by(SV_ID, chr, pos, type, len) %>%
  summarise(homo_ref=sum(genotype==0, na.rm=TRUE), 
            het=sum(genotype==1, na.rm=TRUE), 
            homo_var=sum(genotype==2, na.rm=TRUE)) %>% 
 mutate(enough_data=ifelse(sum(c(homo_ref,het,homo_var))>=6, "yes","no")) %>% 
 filter(enough_data == "yes") %>% filter(homo_var >=6) %>% 
 ungroup() %>% select(SV_ID) %>% unlist() %>% as.vector() -> homo_var_jd_clade
  
filtered_table %>% 
  filter(individual %in% jd_clade) %>%  
  group_by(SV_ID, chr, pos, type, len) %>%
  summarise(homo_ref=sum(genotype==0, na.rm=TRUE), 
            het=sum(genotype==1, na.rm=TRUE), 
            homo_var=sum(genotype==2, na.rm=TRUE)) %>% 
 mutate(enough_data=ifelse(sum(c(homo_ref,het,homo_var))>=6, "yes","no")) %>% 
 filter(enough_data == "yes") %>% filter(homo_ref >=6) %>% 
 ungroup() %>% select(SV_ID) %>% unlist() %>% as.vector() -> homo_ref_jd_clade

# Now the same for the crow clade, allowing for 3 errors:  
  filtered_table %>% 
    filter(individual %in% crows) %>%  
    group_by(SV_ID, chr, pos, type, len) %>%
    summarise(homo_ref=sum(genotype==0, na.rm=TRUE), 
              het=sum(genotype==1, na.rm=TRUE), 
              homo_var=sum(genotype==2, na.rm=TRUE)) %>% 
    mutate(enough_data=ifelse(sum(c(homo_ref,het,homo_var))>=21, "yes","no")) %>% 
    filter(enough_data == "yes") %>% filter(homo_ref >=21) %>% 
    ungroup() %>% select(SV_ID) %>% unlist() %>% as.vector() -> homo_ref_crow_clade
  
filtered_table %>% filter(SV_ID %in% homo_ref_crow_clade ) %>% filter(individual == 2) %>%
  select(SV_ID) %>% unlist() %>% as.vector() -> jd_clade_variants
  
filtered_table %>% filter(SV_ID %in% homo_var_jd_clade | SV_ID %in% homo_ref_jd_clade) %>%
  filter(individual == 2) %>%
  select(SV_ID) %>% unlist() %>% as.vector() -> crow_clade_variants
  
filtered_table %>% filter(SV_ID %in% crow_clade_variants | SV_ID %in% jd_clade_variants) %>% 
  filter(individual == 2) %>%
  select(SV_ID) %>% unlist() %>% as.vector() -> phylo_filtered_variants
  
filtered_table %>% filter(SV_ID %not_in% phylo_filtered_variants) %>% filter(individual == 2) %>%
  select(SV_ID) %>% unlist() %>% as.vector() -> polymorphic_across_clades
  
``` 

In the next step, we determined whether variant type, distance to chromosome end and repeat density are associated with removed variants:
```{R, eval = FALSE}
# Determine distance to chromosome end:

# first use one individual in the table :

filtered_table %>% filter(individual == 2) -> input

# Load a bed file with 1 Mb windows
win<-read.table("1Mb_windows.genome.HCv5.5.bed", header=F)
colnames(win)<-c("chr", "start", "end")
chr_lengths<-read.table("HCv5.5.genome_file.txt", header=T)

# Load RepeatMasker bed file
repeat_masker_track<-read.table("repeatmasker_HCv5.5.bed", header=F)
names(repeat_masker_track)<-c("chr", "start", "end", "ID", "family")

# Run a loop to get repeat density per window
rep_density<-c()
for (i in 1:length(win$chr)){
  repeat_masker_track %>% filter(chr==paste0(win[i,1]) & 
                                   start >= win[i,2] & 
                                   end <= win[i,3]) %>% dim() -> temp
  rep_density[i]<-temp[1]
}
win<-cbind(win, rep_density) 
distance_to_chr_end<-c()
  for (i in 1:length(win$chr)){
    print(win$chr[i] )
    print(win$start[i])
    if (win$start[i] < chr_lengths %>% filter(ID == paste(win$chr[i])) %>%
        select(length) %>% unlist() %>% as.vector() / 2){
    distance_to_chr_end[i]<-win$start[i] 
    }
    else{
      distance_to_chr_end[i]<- chr_lengths %>%
        filter(ID == paste(win$chr[i])) %>% 
        select(length) %>% unlist() %>% as.vector() - win$start[i]
    }
  }
win <- cbind(win, distance_to_chr_end)
win %>% mutate(distance_to_chr_end_Mb = distance_to_chr_end / 1000000) -> temp
win <- temp

# Plot repeat density vs. relative distance to chromosome end
win %>%mutate(ID=chr) %>% 
  merge(., chr_lengths, by="ID") %>% 
  mutate(rel_dist_to_chr_end=distance_to_chr_end/(length/2)) %>% 
  ggplot(aes(rel_dist_to_chr_end, rep_density)) + 
   theme_classic(base_size = 9) +
   geom_point() + 
   geom_smooth() +
   xlab("Relative distance to chromosome end") +
   ylab("Repeat density per 1-Mb window")  -> distance_to_chr_end_vs_rep_density

# Define a function to get the repeat density per site
get_repeats<-function(chrID,posID){
 tem_table<-win %>%
   filter(chr == chrID & as.numeric(start) <= as.numeric(posID) &
            as.numeric(end) >= as.numeric(posID)) 
 if(dim(tem_table)[1]!=0){
    return(tem_table$rep_density)
  } else {
    return(0)
  }
}

# Run over all chromosomes
rep_density_per_site<-c()
for (i in 1:length(input$chr)){
  print(input$pos[i])
  if (input$pos[i] < chr_lengths %>% 
      filter(ID == paste(input$chr[i])) %>% 
      select(length) %>% unlist() %>% as.vector() / 2){
  distance_to_chr_end[i]<-input$pos[i] 
  }
  else{
    distance_to_chr_end[i]<- chr_lengths %>%
      filter(ID == paste(input$chr[i])) %>% 
      select(length) %>% unlist() %>% as.vector() - input$pos[i]
  }
 rep_density_per_site[i]<- get_repeats(chrID = paste0(input$chr[i]),
                                       posID = as.numeric(input$pos[i]))
}

# Split in two tables, one for kept and one for removed variants
input <- cbind(input, rep_density_per_site)
input %>% mutate(distance_to_chr_end)  %>% 
  filter(SV_ID %in% polymorphic_across_clades ) -> GTs_filtered_out
input %>% mutate(distance_to_chr_end) %>% 
  filter(SV_ID %not_in%  polymorphic_across_clades) -> GTs_kept

GTs_kept$keptYN <- 1
GTs_filtered_out$keptYN <- 0
dat_GT <- rbind(GTs_kept,GTs_filtered_out)
dat_GT$DistanceToEndMB <- dat_GT$distance_to_chr_end / 1000000
dat_GT$lenMB <- dat_GT$len / 1000000

# Run Linear models to assess the influence of Distance to 
# chromosome end, variant type and repeat density on the variant filter
mod1 <- glm(keptYN ~ DistanceToEndMB + factor(type), data=dat_GT, family=binomial)
summary(mod1)

mod1 <- glm(keptYN ~ rep_density_per_site, data=dat_GT, family=binomial)
summary(mod1)

mod2 <- glm(keptYN ~ DistanceToEndMB + factor(type), data=dat_GT, family=quasibinomial)
summary(mod1)
summary(mod2)
mod3 <- glmer(keptYN ~ scale(DistanceToEndMB) + factor(type) + (1|chr), data=dat_GT, family=binomial)
  summary(mod3)
   
# Produce figures
  
colors<-c("red", "black")
n<-c(0,1)
names(colors)<-n

dat_GT %>% filter(type != "DUP") %>% 
  ggplot(aes(type)) + 
  geom_bar(aes(fill=as.factor(keptYN))) +
  ylab("Count") +
  xlab("SV Class") +
  scale_fill_manual(breaks=c("0", "1"), 
                    values=c("red", "black"), 
                    labels=c("Genotypes filtered", 
                             "Genotypes kept"), name="" ) +
  theme_classic(base_size=9) +
  theme( legend.justification=c(1,1),  
         legend.position="none") -> phylo_filter_variant_class

dat_GT %>%  mutate(ID=chr) %>% 
  merge(., chr_lengths, by="ID") %>% 
  mutate(rel_dist_to_chr_end=distance_to_chr_end/(length/2)) %>%
ggplot(aes(rel_dist_to_chr_end)) +
  theme_classic(base_size = 9) + 
   geom_density(aes(group=keptYN, colour = as.factor(keptYN)) ) +
   scale_colour_manual( breaks=c("0", "1"), 
                        values=c("red", "black"), 
                        labels=c("Variants filtered", "
                                 Variants kept"), name="") +
   ylab("Density") + xlab("Relative distance to chromosome end") +
   theme(legend.justification=c(1,1), 
         legend.position="none") -> phylo_filter_density_plot


pdf("Figure_2B.pdf", width = 26, height = 8)
svg("Figure_2B_2019-09-30.svg", width=8, height=2.5)
grid.arrange(phylo_filter_variant_class, 
             phylo_filter_density_plot,
             distance_to_chr_end_vs_rep_density, ncol=3)
dev.off()
```

### SV lengths by LR and OM

Here we plotted the lengths of SVs identified with LR and OM:
```{R, eval = FALSE}
dat<-read.table("OM_LR_SVs_2019-09-16", header=F)
names(dat) <- c("chr", "pos", "source", "len", "type")
dat %>% filter(type %in% c("INS", "DEL", "INV")) -> temp
dat <- temp
summary(dat)

xlabel<-"Length (bp)"
ylabel<-"Count"
filtered_table %>% filter(individual == 2) %>% 
  filter(type != "INV")  %>%
  filter(SV_ID %not_in% polymorphic_across_clades) %>% 
  select(len, type) %>% 
  mutate(source="LR") ->  lr_svlen_table
dat %>% filter(type != "INV") %>%  
  filter(source %in% c("OM", "OM_and_LR")) %>%
  filter(len > 1000) %>% 
  select(len, type) %>% 
  mutate(source="OM") %>% rbind(., lr_svlen_table) %>%  
  ggplot(aes(len, fill=type, color=type)) + 
  geom_histogram(binwidth = 20) +
  xlab(xlabel)+ 
  ylab(ylabel) + 
  xlim(0,10000) + theme_classic(base_size= 9) + scale_y_log10() + 
  facet_grid( rows = vars(source), scales="free_y") + 
  theme(strip.background = element_blank(), legend.position = "none",
        axis.text.y=element_text(size = 9, colour="black"),
        axis.text.x=element_text(size = 9, colour="black")) -> LR_OM_readlength

svg("Supplementary_Figure_INV_length_dist.svg" , width=3.5, height=3.5)
xlabel<-"Length (bp)"
ylabel<-"Count"
filtered_table %>% 
  filter(individual == 2) %>% 
  filter(type == "INV")  %>%
  filter(SV_ID %not_in% polymorphic_across_clades) %>% 
  select(len, type) %>% mutate(source="LR") ->  lr_svlen_table
dat %>% filter(type == "INV") %>%  
  filter(source %in% c("OM", "OM_and_LR")) %>%
  filter(len > 1000) %>% select(len, type) %>%
  mutate(source="OM") %>% rbind(., lr_svlen_table) %>%  
  ggplot(aes(len, fill=type, color=type)) + geom_histogram(binwidth = 20) +
  xlab(xlabel)+ ylab(ylabel) + xlim(0,10000) + theme_classic(base_size= 9) + 
  facet_grid( rows = vars(source), scales="free_y") +
  theme(strip.background = element_blank(),
        legend.position = "none", 
        axis.text.y=element_text(size = 9, colour="black"),
        axis.text.x=element_text(size = 9, colour="black")) 
dev.off()

```

### Repeat characterization of LR insertions and deletions:

```{R, eval = FALSE}
rm_output<-read.table("rm_pipeline_output_updated_with_repIDs_correctIDs")
colnames(rm_output) <- c("SV_ID", "repID", "repeat_class") 
rm_output %>% filter(SV_ID %not_in% exclude_variants) -> temp
rm_output <- temp

filtered_table %>% filter(individual == 2) %>% filter(type != "INV")   %>% select(SV_ID) %>%
  filter(SV_ID %in% crow_clade_variants | SV_ID %in% jd_clade_variants) %>%
  merge(., rm_output, by="SV_ID", all=TRUE) %>% 
  mutate(repeat_class=factor(ifelse(is.na(repeat_class),
                                    "no_hit",as.vector(repeat_class))))-> rm_output

rm_output %>% filter(SV_ID %in% crow_clade_variants |
                       SV_ID %in% jd_clade_variants) -> phylofiltered_rm_output

plyr::count(phylofiltered_rm_output$repeat_class) %>%
  mutate(repeat_class=factor(ifelse(freq < 700,"Other",as.vector(x)))) %>%
  arrange(desc(freq)) -> z_filtered

col<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c",
       "#b2df8a","#b2df8a", "#b2df8a", "#b2df8a","#b2df8a",
       "#b2df8a","#b2df8a","#b2df8a")
n<-c("no_hit","LTR", "Simple_repeat" , "Low_complexity",
     "LINE/CR1","Other","Other","Other",
     "Other","Other","Other" ,"Other" ,"Other")
names(col)<-n

z_filtered %>% 
  filter(repeat_class == "Other") %>% 
  select(freq) %>%
  sum() -> other_sum
z_filtered %>% 
  filter(repeat_class!="Other") %>% 
  add_row(., x="Other", freq=other_sum, repeat_class="Other") -> temp
rep_type<-c("none", "ir", "tr", "tr", "ir", "tr", "ir", "unknown", "tr", "tr", "ir", "tr", "tr")

ggplot(temp, aes(x="", y=freq, fill=repeat_class)) +
 theme_classic(base_size = 9) + 
 geom_bar(width = 1, stat = "identity") +
 coord_polar("y", start=90) +
 geom_text(aes(y = freq/6 + c(0, cumsum(freq)[-length(freq)]), 
 label = freq), size=3) + theme(legend.position = "bottom", 
 axis.title.x=element_blank(), 
 axis.text.x=element_blank()) + scale_fill_manual(values=col) -> rm_output_figure

 svg("figure_3ab_2019-10-17.svg", width= 7, height=3)
 grid.arrange(LR_OM_readlength, rm_output_figure, ncol=2)
 dev.off()

 # Look which individual repeat motifs are most common:

# load lengths of individual repeat motifs
rep_lengths<-read.table("rep_lengths", header= F) 
names(rep_lengths)<- c("repID", "length")
head(rep_lengths) 

# determine frequency of single repeat motifs 
phylofiltered_rm_output %>% select(repID) %>% 
plyr::count() %>% arrange(desc(freq)) %>% 
merge(., phylofiltered_rm_output, by="repID", all = T) %>%
  merge(., rep_lengths, by="repID", all = T) %>%
  arrange(desc(freq)) %>%
filter(repeat_class != "no_hit") %>%  
  select(repID, freq, repeat_class, length) %>% 
unique() %>%  filter(freq > 100)   %>% 
  write.table(., file="freq_over_100.txt", sep ="\t", quote=FALSE, row.names = FALSE)

phylofiltered_rm_output %>% select(repID) %>% 
plyr::count() %>% arrange(desc(freq)) %>% 
merge(., phylofiltered_rm_output, by="repID", all = T) %>%
  merge(., rep_lengths, by="repID", all = T) %>%  arrange(desc(freq)) %>%
filter(repeat_class != "no_hit") %>%  select(repID, freq, repeat_class, length) %>% 
unique() %>%  head(5) %>% select(freq) %>% sum() / 21158
```

### Site frequency spectrum of LR variants

```{R, eval = FALSE}

four_most_common_repeat_classes<-c("no_hit", "LTR", "Simple_repeat", "Low_complexity", "LINE/CR1")

# SFS plots for binned populations - Crow clade
    
col<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , 
       "#b2df8a","#b2df8a", "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
n<-c("no_hit","LTR", "Simple_repeat" , "Low_complexity" ,
     "LINE/CR1","Other","Other","Other","Other","Other","Other" ,"Other" ,"Other")
names(col)<-n

filtered_table %>% filter(type != "INV") %>%  filter(SV_ID %in% crow_clade_variants) %>%
  filter(individual %in% c(hc, cc, cce, ac)) %>%   
  group_by(SV_ID) %>% summarize(freq=sum(genotype)) %>% 
  filter(freq!=0 & freq!=48) %>% mutate(folded_freq=ifelse(freq>=25,48-freq,freq)) %>%
  join(., rm_output, by="SV_ID") %>% filter(repeat_class %in% four_most_common_repeat_classes) %>%
  ggplot(aes(folded_freq, stat(density), fill=repeat_class)) + geom_histogram(binwidth=2) +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  scale_fill_manual(values = col) +
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(angle=45, vjust=0.5),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())+
  facet_grid(. ~ repeat_class,
             scale="free_x",
             space="free_x") -> plot
ggplot_build(plot) -> pg

col<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , "#b2df8a","#b2df8a",
       "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
n<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , "#b2df8a","#b2df8a",
     "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
names(col)<-n
titles<-c("LINE/CR1", "Low complexity", "LTR", "No match", "Simple repeat")
n<-c(1,2,3,4,5)
names(titles)<-n

pg$data[[1]] %>% mutate(chr_nr = x+0.5) -> temp
temp$group<-factor(temp$group,levels= c(1,3,2,5,4))

labels<-c("0.04", "", "0.12", "", "0.21", "", "0.29", "", "0.38", "", "0.46", "")
temp %>%
  ggplot(aes(as.factor(chr_nr), y=density, fill=fill)) + geom_bar(stat= "identity") +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  scale_x_discrete(labels=labels)+
  scale_fill_manual(values = col) +
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text( angle=75, vjust=0.5),
        axis.title.y=element_text(),
        strip.text.x = element_text(size=9, margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())+
  facet_grid(. ~ group,
             labeller=labeller(group = titles),
             scale="free_x",
             space="free_x") ->sfs_crow_clade_indels

filtered_table %>% filter(type == "INV") %>%  
  filter(SV_ID %in% crow_clade_variants) %>%
  filter(individual %in% c(hc, cc, cce, ac)) %>%   
  group_by(SV_ID) %>% summarize(freq=sum(genotype)) %>% 
  filter(freq!=0 & freq!=48) %>%
  mutate(folded_freq=ifelse(freq>=25,48-freq,freq)) %>%
    ggplot(aes(folded_freq, stat(density))) + geom_histogram(binwidth=2) +
    theme_classic(base_size = 9) +
        ylab("Frequency")+
      theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(angle=45),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
          strip.background = element_blank()) -> plot
      ggplot_build(plot) -> pg

pg$data[[1]] %>% mutate(chr_nr = x+0.5) -> temp

temp %>%
ggplot(aes(as.factor(chr_nr), y=density)) + geom_bar(stat= "identity") +
    theme_classic(base_size = 9) +
        scale_x_discrete(labels=labels)+
        ggtitle("Inversion") +
      theme(plot.margin = unit(c(5,5,5,2),"mm"),
        panel.spacing.x = unit(2,"mm"),
        plot.title = element_text( size=9,hjust=0.5),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text( angle=75, vjust=0.5),
        axis.title.y=element_blank(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
          strip.background = element_blank())  -> sfs_crow_clade_inversion

svg("sfs_crow_clade_binned.svg",  width= 7, height=3)
grid.arrange(sfs_crow_clade_indels, sfs_crow_clade_inversion, ncol=2, widths=c(5,1.3))
dev.off()

# SFS plots for binned populations - Jackdaw clade

col<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , "#b2df8a",
       "#b2df8a", "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
n<-c("no_hit","LTR", "Simple_repeat" , "Low_complexity" ,"LINE/CR1",
     "Other","Other","Other","Other","Other","Other" ,"Other" ,"Other")
names(col)<-n

filtered_table %>% filter(type != "INV") %>%  
  filter(SV_ID %in% jd_clade_variants) %>% 
  filter(individual %in% jd_clade) %>%   
  group_by(SV_ID) %>% summarize(freq=sum(genotype)) %>%
  filter(freq!=0 & freq!=16) %>% mutate(folded_freq=ifelse(freq>=9,16-freq,freq)) %>% 
  join(., rm_output, by="SV_ID") %>% 
  filter(repeat_class %in% four_most_common_repeat_classes) %>%
  ggplot(aes(folded_freq, stat(density), fill=repeat_class)) +
  geom_histogram(binwidth=2) +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  scale_fill_manual(values = col) +
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(angle=45, vjust=0.5),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())+
  facet_grid(. ~ repeat_class,
             scale="free_x",
             space="free_x") -> plot
ggplot_build(plot) -> pg

col<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , "#b2df8a",
       "#b2df8a", "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
n<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , "#b2df8a",
     "#b2df8a", "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
names(col)<-n
titles<-c("LINE/CR1", "Low complexity", "LTR", "No match", "Simple repeat")
n<-c(1,2,3,4,5)
names(titles)<-n

pg$data[[1]] %>% mutate(chr_nr = x+0.5) -> temp
temp$group<-factor(temp$group,levels= c(1,3,2,5,4))

temp %>%
  ggplot(aes(as.factor(chr_nr), y=density, fill=fill)) +
  geom_bar(stat= "identity") +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  scale_x_discrete(labels=round(seq(2,8,2)/16, digits=2))+
  scale_fill_manual(values = col) +
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(size=9, angle=75, vjust=0.5),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())+
  facet_grid(. ~ group,
             labeller=labeller(group = titles),
             scale="free_x",
             space="free_x") ->sfs_jackdaw_clade_indels

filtered_table %>% filter(type == "INV") %>%
  filter(SV_ID %in% jd_clade_variants) %>% 
  filter(individual %in% jd_clade) %>%   
  group_by(SV_ID) %>% summarize(freq=sum(genotype)) %>% 
  filter(freq!=0 & freq!=16) %>% 
  mutate(folded_freq=ifelse(freq>=9,16-freq,freq)) %>%
  ggplot(aes(folded_freq, stat(density))) + geom_histogram(binwidth=2) +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(angle=45),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank()) -> plot
ggplot_build(plot) -> pg

pg$data[[1]] %>% mutate(chr_nr = x+0.5) -> temp

temp %>%
  ggplot(aes(as.factor(chr_nr), y=density)) + geom_bar(stat= "identity") +
  theme_classic(base_size = 9) +
  scale_x_discrete(labels=round(seq(2,8,2)/16, digits=2))+
  ggtitle("Inversion") +
  theme(plot.margin = unit(c(5,5,5,2),"mm"),
        panel.spacing.x = unit(2,"mm"),
        plot.title = element_text(size=9, hjust=0.5),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(size=9, angle=75, vjust=0.5),
        axis.title.y=element_blank(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())  -> sfs_jackdaw_clade_inversion

svg("sfs_jd_clade_binned.svg",  width= 7, height=3)
grid.arrange(sfs_jackdaw_clade_indels, 
             sfs_jackdaw_clade_inversion, ncol=2, widths=c(5,1))
dev.off()

# SFS for single populations:

# HC SFS
### SFS plots for binned populations:

col<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , 
       "#b2df8a","#b2df8a", "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
n<-c("no_hit","LTR", "Simple_repeat" , "Low_complexity" ,"
     LINE/CR1","Other","Other","Other","Other","Other","Other" ,"Other" ,"Other")
names(col)<-n

filtered_table %>% filter(type != "INV") %>%  
  filter(SV_ID %in% crow_clade_variants) %>% 
  filter(individual %in% hc) %>%   
  group_by(SV_ID) %>% summarize(freq=sum(genotype)) %>%
  filter(freq!=0 & freq!=16) %>% 
  mutate(folded_freq=ifelse(freq>=9,16-freq,freq)) %>% 
  join(., rm_output, by="SV_ID") %>% filter(repeat_class %in% four_most_common_repeat_classes) %>%
  ggplot(aes(folded_freq, stat(density), fill=repeat_class)) +
  geom_histogram(binwidth=2) +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  scale_fill_manual(values = col) +
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(angle=45, vjust=0.5),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())+
  facet_grid(. ~ repeat_class,
             scale="free_x",
             space="free_x") -> plot
ggplot_build(plot) -> pg

col<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , 
       "#b2df8a","#b2df8a", "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
n<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , 
     "#b2df8a","#b2df8a", "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
names(col)<-n
titles<-c("LINE/CR1", "Low complexity", "LTR", "No match", "Simple repeat")
n<-c(1,2,3,4,5)
names(titles)<-n

pg$data[[1]] %>% mutate(chr_nr = x+0.5) -> temp
temp$group<-factor(temp$group,levels= c(1,3,2,5,4))

temp %>%
  ggplot(aes(as.factor(chr_nr), y=density, fill=fill)) + geom_bar(stat= "identity") +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  scale_x_discrete(labels=round(seq(2,8,2)/16, digits=2))+
  scale_fill_manual(values = col) +
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(size=9, angle=75, vjust=0.5),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())+
  facet_grid(. ~ group,
             labeller=labeller(group = titles),
             scale="free_x",
             space="free_x") ->sfs_hc_indels

filtered_table %>% filter(type == "INV") %>%  
  filter(SV_ID %in% crow_clade_variants) %>% filter(individual %in% hc) %>%   
  group_by(SV_ID) %>% summarize(freq=sum(genotype)) %>% 
  filter(freq!=0 & freq!=16) %>% mutate(folded_freq=ifelse(freq>=9,16-freq,freq)) %>%
  ggplot(aes(folded_freq, stat(density))) + geom_histogram(binwidth=2) +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(angle=45),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank()) -> plot
ggplot_build(plot) -> pg

pg$data[[1]] %>% mutate(chr_nr = x+0.5) -> temp

temp %>%
  ggplot(aes(as.factor(chr_nr), y=density)) + geom_bar(stat= "identity") +
  theme_classic(base_size = 9) +
  scale_x_discrete(labels=round(seq(2,8,2)/16, digits=2))+
  ggtitle("Inversion") +
  theme(plot.margin = unit(c(5,5,5,2),"mm"),
        panel.spacing.x = unit(2,"mm"),
        plot.title = element_text(size=9, hjust=0.5),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(size=9, angle=75, vjust=0.5),
        axis.title.y=element_blank(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())  -> sfs_hc_inversion

svg("sfs_hc_binned.svg",  width= 7, height=3)
grid.arrange(sfs_hc_indels, sfs_hc_inversion, ncol=2, widths=c(5,1))
dev.off()

### SFS plots for binned populations:

col<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" ,
       "#b2df8a","#b2df8a", "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
n<-c("no_hit","LTR", "Simple_repeat" , "Low_complexity" ,"LINE/CR1",
     "Other","Other","Other","Other","Other","Other" ,"Other" ,"Other")
names(col)<-n

filtered_table %>% filter(type != "INV") %>%  
  filter(SV_ID %in% crow_clade_variants) %>% filter(individual %in% cc) %>%   
  group_by(SV_ID) %>% summarize(freq=sum(genotype)) %>%
  filter(freq!=0 & freq!=16) %>% mutate(folded_freq=ifelse(freq>=9,16-freq,freq)) %>% 
  join(., rm_output, by="SV_ID") %>%
  filter(repeat_class %in% four_most_common_repeat_classes) %>%
  ggplot(aes(folded_freq, stat(density), fill=repeat_class)) +
  geom_histogram(binwidth=2) +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  scale_fill_manual(values = col) +
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(angle=45, vjust=0.5),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())+
  facet_grid(. ~ repeat_class,
             scale="free_x",
             space="free_x") -> plot
ggplot_build(plot) -> pg

col<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , 
       "#b2df8a","#b2df8a", "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
n<-c("#1f78b4","#ff7f00", "#fdbf6f", "#e31a1c","#33a02c" , 
     "#b2df8a","#b2df8a", "#b2df8a", "#b2df8a","#b2df8a","#b2df8a","#b2df8a","#b2df8a")
names(col)<-n
titles<-c("LINE/CR1", "Low complexity", "LTR", "No match", "Simple repeat")
n<-c(1,2,3,4,5)
names(titles)<-n

pg$data[[1]] %>% mutate(chr_nr = x+0.5) -> temp
temp$group<-factor(temp$group,levels= c(1,3,2,5,4))

temp %>%
  ggplot(aes(as.factor(chr_nr), y=density, fill=fill)) + geom_bar(stat= "identity") +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  scale_x_discrete(labels=round(seq(2,8,2)/16, digits=2))+
  scale_fill_manual(values = col) +
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(size=9, angle=75, vjust=0.5),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())+
  facet_grid(. ~ group,
             labeller=labeller(group = titles),
             scale="free_x",
             space="free_x") ->sfs_cc_indels

filtered_table %>% filter(type == "INV") %>%  
  filter(SV_ID %in% crow_clade_variants) %>% filter(individual %in% cc) %>%   
  group_by(SV_ID) %>% summarize(freq=sum(genotype)) %>% 
  filter(freq!=0 & freq!=16) %>% mutate(folded_freq=ifelse(freq>=9,16-freq,freq)) %>%
  ggplot(aes(folded_freq, stat(density))) + geom_histogram(binwidth=2) +
  theme_classic(base_size = 9) +
  ylab("Frequency")+
  theme(plot.margin = unit(c(5,5,5,5),"mm"),
        panel.spacing.x = unit(2,"mm"),
        title = element_text(),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(angle=45),
        axis.title.y=element_text(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank()) -> plot
ggplot_build(plot) -> pg

pg$data[[1]] %>% mutate(chr_nr = x+0.5) -> temp

temp %>%
  ggplot(aes(as.factor(chr_nr), y=density)) + geom_bar(stat= "identity") +
  theme_classic(base_size = 9) +
  scale_x_discrete(labels=round(seq(2,8,2)/16, digits=2))+
  ggtitle("Inversion") +
  theme(plot.margin = unit(c(5,5,5,2),"mm"),
        panel.spacing.x = unit(2,"mm"),
        plot.title = element_text(size=9, hjust=0.5),
        legend.position="none",
        axis.title.x=element_blank(),
        axis.text.x=element_text(size=9, angle=75, vjust=0.5),
        axis.title.y=element_blank(),
        strip.text.x = element_text( margin=unit(c(2,2,2,2),"mm")), 
        strip.background = element_blank())  -> sfs_cc_inversion

svg("sfs_cc_binned.svg",  width= 7, height=3)
grid.arrange(sfs_cc_indels, sfs_cc_inversion, ncol=2, widths=c(5,1))
dev.off()

```

## Short-read based SV detection

Illumina short reads were mapped with `BWA-MEM` and SV calling was performed on the bam 
files using `Delly`, `Lumpy` and `Manta`. The resulting vcf files were merged with
`SURVIVOR`. 
Here are the representative commands:

```{bash, eval = FALSE}
# Delly
delly call -o delly_sv '_delly' -s 15 -q 20 -g reference mapped_reads.bam  

# Lumpy
lumpy -mw 4 -tt 0.0 -pe bam_file:$out'.discordand.sort.bam',\
histo_file:$out'.hist',mean:$INS,stdev:$STD,read_length:$len, \
min_non_overlap:150,discordant_z:4,back_distance:20,weight:1, \
id:1,min_mapping_threshold:20  -sr bam_file:$out'splitters.sort.bam',\
back_distance:20,weight:1,id:2,min_mapping_threshold:20 > lumpy_sv.vcf

#Manta
out="output_Manta"
echo $out
#create Manta workflow
configManta.py --bam= mapped_reads.bam --referenceFasta= reference --runDir=$out'_Manta'  
#run Manta workflow
python $out'_Manta'/runWorkflow.py -j 15 -m local -g 30 
#combine results from Manta
gunzip  --stdout $out'_Manta/results/variants/candidateSV.vcf.gz' > $out'.manta.vcf'
gunzip  --stdout $out'_Manta/results/variants/diploidSV.vcf.gz' | \
grep -v '^#' >> $out'.manta.vcf' 
gunzip  --stdout $out'_Manta/results/variants/candidateSmallIndels.vcf.gz'| \
grep -v '^#' >> $out'.manta.vcf'
```


# FST analysis

We calculated FST for the LR variants using `vcftools`. First we removed duplicated variants 
(those with the exact same starting position), as those caused issues  with `vcftools`. 

```{bash, eval = FALSE}
grep -v '#' merged_filtered_force_called.vcf | \
   awk '{print $1 "/" $2}' | sort | uniq -c | awk '$1 == 1' | \
   awk '{print $2}' > ref

grep -v '#' merged_filtered_force_called.vcf | \
   tr ";" "\t"  | cut -f 1,2,5,10 | sed -e 's/SVLEN=//g'  | \
   sed -e 's/<//g' -e 's/>//g' | awk '{print $1 "/" $2 "\t" $0}' > temp
````

Small python script to join to files in order:

```{python, eval = FALSE}
import csv
import sys
import io

input_1=sys.argv[1]
input_2=sys.argv[2]
input_dict={}

with open(input_1) as csvfile:
  inputfile = csv.reader(csvfile, delimiter='\t')
  for row in inputfile:
    key = row[0]
    value = row[0:4]
    input_dict[key] = value

with open(input_2) as csvfile:
  inputfile = csv.reader(csvfile, deliminter='\t')
  for row in inputfile:
    if row[0] in input_dict.keys():
      print input_dict[row[0]]
    else:
      pass
```
Use this script to sort remove duplicated variants.
``` {bash, eval = FALSE}
python join_ref.py temp ref | sed -e 's/\[//g' -e 's/\]//g' -e 's/\,//g' |\ 
 tr "'" "X" | sed 's/X//g' | tr " " "\t" > ref_2019-09-16
```

Now calculate FST between all possible populations / species:
``` {bash, eval = FALSE}
while IFS= read -r line; do
  pop1=$(echo $line | awk '{print $1}' )
  pop2=$(echo $line | awk '{print $2}' )
  echo $pop1 $pop2
  vcftools --vcf merged_filtered_force_called.vcf \
     --weir-fst-pop $pop1 
     --weir-fst-pop $pop2
     --out ${pop1}_${pop2}.2019-09-16 \
     --max-missing 0.8
  tail -n +2 ${pop1}_${pop2}.2019-09-16.weir.fst | awk '{print $1 "/" $2 "\t" $3}' | \
  sort -k 1,1 | sed 's/-nan/NA/g' > temp
  python join.py temp ref_2019-09-16  | \
  cat <(echo 'fst_'$pop1'.'$pop2) - > ${pop1}_${pop2}.2019-09-16.out
  rm temp
done<combinations

cat <(echo 'SV_ID       chr     pos     type    len') ref_2019-09-16 | \
paste - *2019-09-16.out
```

This table is then used for the analysis in `R`:

```{R, eval = FALSE}
fst<-read.table("fst_filtered_force_called_B03_excluded_HC_ref.2019-09-16.table", header=T)
fst %>% mutate_at( names(fst)[6:19], funs(ifelse(. < 0, 0, .))) -> temp
fst<-temp
fst %>% mutate(length=sqrt(len^2)) %>% select(-len) %>%
  mutate(len=length) %>% select(-length) -> temp
fst<-temp

# first look at the phenotypically divergent comparison of the
# German all black carrion crow and black-and-grey hooded crow:
fst %>% mutate(SV_ID = paste0(chr, "_" ,pos)) %>%
  filter(SV_ID %in% crow_clade_variants) %>% select(fst_HC.CC) %>%
  unlist() %>% as.vector() %>% quantile(0.99, na.rm=T) -> ninetyninth_percentile_fst_HC.CC
fst %>% mutate(SV_ID = paste0(chr, "_" ,pos)) %>%
  filter(SV_ID %in% crow_clade_variants) %>%
  filter(fst_HC.CC > ninetyninth_percentile_fst_HC.CC ) %>% 
  arrange(desc(fst_HC.CC)) %>% select(chr, pos, type, len, fst_HC.CC) %>%
  mutate(SV_ID =paste0(chr, "_", pos)) -> ninetyninth_percentile_outlier_variants_HC_CC
write.table(ninetyninth_percentile_outlier_variants_HC_CC,
            file="ninetyninth_percentile_outlier_variants_HC_CC.txt", 
            sep="\t", quote=F, row.names = F)

# As a control, compare within within phenotype (black German carrion vs. black Spanish carrion):
fst %>% mutate(SV_ID = paste0(chr, "_" ,pos)) %>% 
  filter(SV_ID %in% crow_clade_variants) %>% 
  select(fst_CC.CCE) %>% unlist() %>% as.vector() %>% 
  quantile(0.99, na.rm=T) -> ninetyninth_percentile_fst_CC.CCE
fst %>% mutate(SV_ID = paste0(chr, "_" ,pos)) %>%
  filter(SV_ID %in% crow_clade_variants) %>% 
  filter(fst_CC.CCE > ninetyninth_percentile_fst_CC.CCE ) %>%
  arrange(desc(fst_CC.CCE)) %>% 
  select(chr, pos, type, len, fst_CC.CCE) %>% 
  mutate(SV_ID =paste0(chr, "_", pos)) -> ninetyninth_percentile_outlier_variants_CC_CCE
write.table(ninetyninth_percentile_outlier_variants_CC_CCE,
            file="ninetyninth_percentile_outlier_variants_CC_CCE.txt", sep="\t")

# Now look at the overlap HC.CC vs. CC.CCE:

ninetyninth_percentile_outlier_variants_HC_CC %>%
  merge(., ninetyninth_percentile_outlier_variants_CC_CCE) %>% 
  select(SV_ID) %>% unlist() %>% as.vector() -> overlap_HC.CC_CC.CCE

ninetyninth_percentile_outlier_variants_HC_CC %>% 
  filter(SV_ID %not_in% overlap_HC.CC_CC.CCE) -> 
  ninetyninth_percentile_outlier_variants_HC_CC_filtered
write.table(ninetyninth_percentile_outlier_variants_HC_CC_filtered,
            "ninetyninth_percentile_outlier_variants_HC_CC_filtered.txt", quote=F, row.names = F)
```

# Gene expression of the _NDP_ gene

Here we took gene expression data from _Poelstra et al. 2019_ and looked
at the expression of the _NDP_ gene in relation to the LTR element insertion genotype. 

```{R, eval = FALSE}
gene_counts<-read.table("transformed_gene_counts", header=T)
col<-c("#bdbdbd", "#636363", "#1f78b4", "#b2df8a", "#33a02c")
name<-c("HC", "CC",0,1,2)
names(col)<-name

# Linear model:
gene_counts %>% filter(part == "body") %>%
  lm(normalized_count ~ -1 + factor(gt), data= . ) -> mod

summary(mod)

svg("Normalized_gene_counts_NDP.svg", height=3, width=3)
gene_counts %>% filter(part=="body") %>% 
   filter(gt != "NA") %>% 
   ggplot(aes(as.factor(gt), normalized_count, fill=as.factor(gt))) + 
   theme_classic(base_size = 9) +
   geom_boxplot() + 
   scale_fill_manual(values=col) +
   ylab("Normalized gene count")+
   scale_x_discrete(breaks=c("0", "1", "2"), 
                    labels=c("Homozygous insertion", 
                             "Heterozygous", 
                             "Homozygous non-insertion")) +
theme(
  legend.position="none",
  axis.text.x = element_text(size = 9),
      axis.title.x=element_blank())
dev.off()
```

# Repeat characterization pipeline

To assign insertion / deletion sequences to known repeats, we developed a pipeline which uses `RepeatMasker`.
In the following `Makefile` all steps are automized:

```{make, eval = FALSE}
SHELL := /bin/bash
MAKEFLAGS += --no-builtin-rules
.SUFFIXES:
VPATH := ./vcfs
VCFS  := $(notdir $(shell ls ${VPATH}/*.vcf.gz))
RLIB  := fAlb_lycPyr_hcrow_aves_combined.lib
#cores to run repeat masker on 
#can not be empty
CORES := 10
.DELETE_ON_ERROR:
.PHONEY:All
All: \
	${VCFS:.vcf.gz=.info}       \
	${RLIB:=.fai}               \
	info_joined.txt             \
	for_repeatmasker.fasta      \
	for_repeatmasker.fasta.fai  \
	for_repeatmasker.key        \
	for_repeatmasker.fasta.out  \
	for_repeatmasker.tab        \
	matrix.res                 \
	filtered_matrix.res       \
	filtered_persite_matrix.res
for_repeatmasker.fasta for_repeatmasker.key:$@
%.info:%.vcf.gz
	#getting SEQ field from file $< using bcftools
	echo -e "CHROM\tPOS\tREF\tALT\tSEQ_$(notdir ${<:.vcf.gz=})" > $@
	bcftools query -f '%CHROM\t%POS\t%REF\t%ALT\t%SEQ\n' $< >> $@
info_joined.txt:$(VCFS:.vcf.gz=.info)
	#pasting and parsing to get sequences in SEQ fields of all files
	echo $^ |sed -s 's/ / \/dev\/null /g' |xargs paste -d $$'\t' > $@
for_repeatmasker.fasta for_repeatmasker.key:get_seq_from_vcf.pl info_joined.txt
	#generating fasta and key files from $(word 2,$^)
	perl $^ $(basename $@)
%.fasta.out:%.fasta  $(RLIB)
	#run repeat masker here on $< with library file ${RLIB}
	RepeatMasker -no_is -pa ${CORES}  -lib $(word 2,$^) $<
%.tab:rmparse.pl %.fasta.out %.fasta.fai ${RLIB:=.fai}
	#parsing repeatmasker output file $< using bioperl
	perl $^ > $@
matrix.res:matmaker.pl for_repeatmasker.tab for_repeatmasker.key info_joined.txt
	#generating tabular multiple sample repeat annottation based 	
	perl $^  > $@
filtered_matrix.res:matfilter.pl matrix.res 
	perl $^ > $@
filtered_persite_matrix.res:persite_matfilter.pl matrix.res
	perl $^ > $@
%.fai:%
	#fai indexing using samtools
	samtools faidx $<
clean:
	rm -f ${VCFS:.vcf.gz=.info}       \
	info_joined.txt                   \
	for_repeatmasker.fasta            \
	for_repeatmasker.key              \
	for_repeatmasker.fasta.out        \
	for_repeatmasker.fasta.cat.gz     \
	for_repeatmasker.fasta.masked     \
	for_repeatmasker.tab              \
	for_repeatmasker.fasta.tbl        \
	matrix.res                        \
	filtered_matrix.res               \
	filtered_persite_matrix.res
``` 
Depending on `gnuparallel`, `BioPerl`, `RepeatMasker` and `samtools`, the pipeline was run with `make`. 


{"mode":"full","isActive":false}















https://github.com/boxu855/GPHbioinformatics

https://github.com/eliza-m








##Genomes and Plant Health 2021 | Workshop for undergraduate research students, July 6th-7th | UC Berkeley (virtual)



July 6th: Genomics and Bioinformatics 

0. Introductions

The site we used for group brain think is: www.mentimeter.com 
Here are results of what “Plant Health” means to us as a group: [link](https://benchling.com/krasileva/f/lib_YvYAlu8K-hvnlrs/etr_FX8JreiY-d7-crisprcas9-enrichment-sequencing-on-nanopore-at5g4373040/edit)

1. Decoding genomes: DNA extraction, sequencing technologies, genome databases
Prof Ksenia Krasileva and China Lunde Shaw

Lecture and tutorial: [link](https://docs.google.com/document/d/1Dq8Z9OCrv0bk4sNEePueMS4BDZhA9zyU/edit)
DNA extraction and loading MinION demo: [link](https://drive.google.com/file/d/1aIZImTIWAIAa1nmXalFIOQFq7ThK8FGE/view?usp=sharing)

Resources:
Phytozome database: https://phytozome.jgi.doe.gov/pz/portal.html 
Ensembl Plants database: https://plants.ensembl.org


2. Genome assembly, annotation, and the ins and outs of computational biology 
Pierre Joubert and Boyan Xu

 Lecture and coding exercises: [link](https://drive.google.com/drive/folders/1oNYLAhAQ7EsugxOCgAQj16OC4hQyM5l3?usp=sharing)
 
 3. From genes to proteins: protein domain architectures and families, structures 
Dr Daniil Prigozhin

Lecture: [link](https://drive.google.com/file/d/1UBIDCPLXV0gO5ea6q9zPqB1XGnMOeqrP/view?usp=sharing) 
Exercises: [link](https://docs.google.com/document/d/1de8vx9OB3X46aJMCsjfiGgBaBVw80i4BEHEwVBY0e2I/edit)

Resources:
UCSF Chimera: https://www.cgl.ucsf.edu/chimera/ 



July 7th: Applications to Plant Health 

4. Genome editing 
Dr Michael Gomez and Prof Ksenia Krasileva

Lecture and tutorial: [link](https://drive.google.com/file/d/1msnNFrx1rvWw1gpZM69pNJ2Q7r97RThw/view?usp=sharing)

Resources:
CRISPR 2.0: http://crispr.hzau.edu.cn/CRISPR2/  

5. Good microbes, bad microbes: plant microbiomes
Reena Debray with tech support by Pierre Joubert 

Lecture and coding exercise: [link](https://drive.google.com/drive/folders/16q_PfDH5E31Gcm3aEG5Vz8_L16rxmkDp?usp=sharing)

6. Panel career discussion

UC Berkeley Plant Biology graduate program: https://pmb.berkeley.edu/graduate-programs/plant-biology 

UC Berkeley Microbiology graduate program:
https://pmb.berkeley.edu/graduate-programs/microbial-biology 

UC Berkeley Integrative Biology graduate program:
https://ib.berkeley.edu/grad 

UC Berkeley Center for Computational Biology graduate program:
https://ccb.berkeley.edu/academics/phd-in-computational-biology/ 

UC Berkeley Center for Computational Biology Designated Emphasis program:
https://ccb.berkeley.edu/academics/designated-emphasis/

LBNL undergraduate and graduate student internships:
https://education.lbl.gov/internships/ 

Norwich Research Park Graduate Program:
https://biodtp.norwichresearchpark.ac.uk 

7. A Computational Biologist Toolkit

Google Colab: [link](https://colab.research.google.com/notebooks/intro.ipynb?utm_source=scs-index)
BASH programming how to: [link](https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html)
What is R?: [link](https://www.r-project.org/about.html)
Python for beginners: [link](https://www.python.org/about/gettingstarted/)
Snakemake workflows: [link](https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html)
Github (version control code management): [link](https://guides.github.com/activities/hello-world/)
Zenodo (storage and distribution of data analyses with DOI): [link](https://zenodo.org/communities/)   

 
Our contact information
Prof Ksenia Krasileva, Email: kseniak@berkeley.edu   Twitter: @kseniakrasileva
China Lunde Shaw,  Email: lundec@berkeley.edu Twitter: @SciChi007
Pierre Joubert, Email: pierrj@berkeley.edu  Twitter: @pmjoubert
Boyan Xu, Email: boxu@berkeley.edu 
Dr Michael Gomez, Email: michaelgr@berkeley.edu  Twitter: @MichaelGzRs
Reena Debray, Email: rrdebray@berkeley.edu  Twitter: @DebrayReena
Dr Daniil Prigozhin, Email: daniilprigozhin@lbl.gov  Twitter: @daniilprigozhin























# Pfam_scan



## Install using bioconda


```python

conda create --name pfam_scan_env

source activate pfam_scan_env


conda install -c bioconda pfam_scan


```



## Download Pfam-A.hmm.gz, Pfam-B.hmm.gz, Pfam-A.hmm.dat.gz,  Pfam-B.hmm.dat.gz, active_site.dat.gz



To find and download those files, do:

- wget http://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam27.0/Pfam-B.hmm.gz for Pfam-B.hmm
- wget http://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam27.0/Pfam-B.hmm.dat.gz for Pfam-B.hmm.dat
- wget http://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.gz for Pfam-B.hmm
- wget http://ftp.ebi.ac.uk/pub/databases/Pfam/current_release/Pfam-A.hmm.dat.gz for Pfam-B.hmm.dat
- wget http://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam33.1/active_site.dat.gz for active_site.dat


of course, need to unzip thoses file before usage by doing `gunzip file.gz`


## Prerepare the file by doing



```python

hmmpress Pfam-A.hmm
hmmpress Pfam-B.hmm

```



## Then run


```python

pfam_scan.pl -fasta my.protein.sequences -cpu 32 -outfile outputname.txt - as -dir fullpathtopfamdatathatwedownloaded 


```


That is it! Please check [Olga gis.github](https://gist.github.com/olgabot/f65365842e27d2487ad3) for manual installation and run











[Gene Ontology Meta Annotator for Plants (GOMAP)](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-021-00754-1#Fig5) | [Running GoMap](https://bioinformapping.com/gomap/master/RUNNING.html)











Standards: Earth BioGenome Project (EBP)


https://www.earthbiogenome.org/sample-collection-processing-standards

https://www.earthbiogenome.org/assembly-standards

https://www.earthbiogenome.org/annotation

https://www.earthbiogenome.org/analysis-standards-report

https://www.earthbiogenome.org/it-and-informatics-standards

https://www.earthbiogenome.org/subcommittee-ethical-legal-social

https://www.earthbiogenome.org/diversity-equity-inclusion-and-justice-committee

https://www.earthbiogenome.org/subcommittee-communications-public-affairs

https://www.earthbiogenome.org/analysis-standards-report


















[phasebook: haplotype-aware de novo assembly of diploid genomes from long reads](https://www.biorxiv.org/content/10.1101/2021.07.02.450883v1)


[github](https://github.com/phasebook/phasebook)

















Just found on twitter an R shiny tool for plant genome annotation...faster...low requirement 8GB RAM and 64 CPU ..on mac and linux... This is...[Hayai-Annotation Plants](https://academic.oup.com/bioinformatics/article/35/21/4427/5489908?login=true#.XcDhT9RUIp0.twitter). [github](https://github.com/kdri-genomics/Hayai-Annotation-Plants) | [wiki](https://github.com/kdri-genomics/Hayai-Annotation-Plants/wiki) | [tutorial](https://github.com/kdri-genomics/Hayai-Annotation-Plants/blob/08d58c3f2758b9fda4da00377087a92c9ef4317e/Hayai-Annotation%20Plants%20Tutorial.pdf)

![img](https://oup.silverchair-cdn.com/oup/backfile/Content_public/Journal/bioinformatics/35/21/10.1093_bioinformatics_btz380/1/btz380f1.jpeg?Expires=1628319577&Signature=yvyVLQJS9S3RB40JaTa6N3E9SABiFEQkR-ywBwHZ8Kf7tcPsMZfcVqbix1pciGpxsoWBD6Ex34pUmEpGup5gNJYEy2rhoT-dKnW8n12F2HV0M4ZjTc7UIpY3rfYzLw5xel~f6ZbR2U2feB~N-V9kk9r2nIq3a5rvAi-zBdo2H-xxerOvOgixBd7TpOUjcmZCyrtk-ao5nvuqMx1jIcNQM47eaPvPsXgMAzybe8z~u923~1HTOWGddD0tXLxs99R0lzTGsdfIumDYmBABU1EjQ9vNnQs1WmS8i4i11xABq9ocsTZQGOY8CajlB24xdTNU1nzp88UuCqzSBvUW1zVAsQ__&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA)



















Excellent [youtube channel](https://youtu.be/cp3yIARAyEI) resource for GWAS







I found the [Korean Post Genome Project output website](http://kpgp.or.kr/bbs/board.php?bo_table=result&page=2)



Awesome tutorial on [AG2PI Workshop #4 - A Practical Guide to Genome-Wide Association Studies (GWAS)](https://youtu.be/nrbgly0Bcv8) | [code github](https://github.com/zhzheng92/AG2PI_GWAS_workshop_June2021) | [PPT](https://github.com/Yedomon/bioinformatics101/blob/main/AG2PI_WS4_20210624_Zheng.pdf)
 
 
 Found Zihao  Zheng | Mail: `zhzheng@iastate.edu` | [github](https://github.com/zhzheng92?tab=repositories) | Twitter: `@zhzheng` | 





New genome annotation tools FINDER | [github](https://github.com/sagnikbanerjee15/Finder) | [Paper](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-021-04120-9) 



The bible of bioinformatic [online](https://bioinformaticsworkbook.org/dataAnalysis/GenomeAnnotation/annotation_and_assembly_index.html#gsc.tab=0)



How to [scaffold](https://bioinformaticsworkbook.org/dataAnalysis/GenomeAssembly/Hybrid/Juicer_Juicebox_3dDNA_pipeline#gsc.tab=0)



[Sebastian Schmeier](https://www.sschmeier.com/index.html) |  Massey University | My ideol in Bioinfo | Training | [Computational Genomics](https://genomics.sschmeier.com/) | [Reproducibility](https://reproducibility.sschmeier.com/) | 


## Blast running for syntheny analysis



```bash

#!/bin/bash

set -e

source activate diamond_env # Activate diamond conda environment

# Step 0: Make database for each species

for prot in *.fasta

	do  
		base=$(basename $prot .fasta)
		
		diamond makedb --in ${base}.fasta  -p 64 -d ${base}.db
	done

# Step 1: Perform the blast 

for db in *.db.dmnd

	do
		base=$(basename $db .db.dmnd)
		diamond blastp -d ${base}.db.dmnd -q at.fasta -p 64 --evalue 0.00001 --out at_vs_${base}.csv --outfmt 6
		diamond blastp -d ${base}.db.dmnd -q sl.fasta -p 64 --evalue 0.00001 --out sl_vs_${base}.csv --outfmt 6
		diamond blastp -d ${base}.db.dmnd -q pc.fasta -p 64 --evalue 0.00001 --out pc_vs_${base}.csv --outfmt 6
		diamond blastp -d ${base}.db.dmnd -q si.fasta -p 64 --evalue 0.00001 --out si_vs_${base}.csv --outfmt 6
		diamond blastp -d ${base}.db.dmnd -q mg.fasta -p 64 --evalue 0.00001 --out mg_vs_${base}.csv --outfmt 6
	done
	
```








Then define a code for picking up only top 5 blastp based on bitscore. do this for each blast result seperately



```bash

#!/bin/bash

set -e


# Remove duplicate and get gene list

cat bae.blast | awk '{ print $1 }' | awk '!seen[$0]++' > uniq

# Pick top 5 genes based on the value o the colum 2

cat uniq | while read line
do
    grep "${line}" bae.blast | sort -r -n -k2 | head -n 5 > bae_out.${line}
done


# Concatenate all 


cat bae_out* > bae_top5.blast


# End


```






Then concatenate all blast top 5 results:


```bash

cat at_vs_at.csv at_vs_sl.csv at_vs_pc.csv at_vs_si.csv at_vs_mg.csv \
sl_vs_sl.csv sl_vs_at.csv sl_vs_pc.csv sl_vs_si.csv sl_vs_mg.csv \
pc_vs_pc.csv pc_vs_at.csv pc_vs_sl.csv pc_vs_si.csv pc_vs_mg.csv \
si_vs_si.csv si_vs_at.csv si_vs_sl.csv si_vs_pc.csv si_vs_mg.csv \
mg_vs_mg.csv mg_vs_at.csv mg_vs_sl.csv mg_vs_pc.csv mg_vs_si.csv > bae.blast

                                   

```






Convert the csv format of the prepared simplified gff into a tabulate format before running mcscan. So do:


```bash

sed 's|,|\t|g' bae.gff.csv > bae.gff

```




Then run MCScanX


I put bae.gff and bae.blast files in a directory called syn

I move to the syn directory and then,


```bash

MCScanX bae

```


To visualize goto https://synvisio.github.io/ . An other option is https://accusyn.usask.ca/ and https://plantinformatics.io/


Quelque refs




[Multiple wheat genomes reveal global variation in modern breeding](https://www.nature.com/articles/s41586-020-2961-x)

> Pairwise BLASTn comparisons of the genes were also used to identify structural variants, and were uploaded into AccuSyn (https://accusyn.usask.ca/) and SynVisio (https://synvisio.github.io/#/) to create a wheat-specific database (https://kiranbandi.github.io/10wheatgenomes/). Pretzel (https://github.com/plantinformatics/pretzel) was also used to visualize and compare the RQA and the projected gene annotations (http://10wheatgenomes.plantinformatics.io/).


[Genome assembly of Chiococca alba uncovers key enzymes involved in the biosynthesis of unusual terpenoids ](https://academic.oup.com/dnaresearch/article/27/3/dsaa013/5869045?login=true)


> To identify homologous genes for synteny analysis, BLASTP (BLAST+ v2.6.0)42 was run using the longest peptide isoform for each gene, comparing C. alba to itself, Coffea canephora to itself and C. alba versus C. canephora in both directions. BLAST hits were filtered for E-value ≤ 1e-5 and only the top five hits (based on bit score) for each query sequence were retained. These BLAST results were used to identify syntenic blocks using MCScanX v2017032255 with default parameters and visualized using SynVisio (https://synvisio.github.io). 



[A chromosome-level genome assembly of the woolly apple aphid, Eriosoma lanigerum Hausmann (Hemiptera: Aphididae)](https://onlinelibrary.wiley.com/doi/full/10.1111/1755-0998.13258)

> Syntenic blocks of genes were identified between the chromosome-level genome assemblies of WAA, M. persicae, A. pisum and R. maidis (see Table S3 for details of assembly and annotation versions used) using mcscanx version 1.1 (Wang et al., 2012). For each comparison, we carried out an all versus all blast search of annotated protein sequences using blastall version 2.2.22 (Altschul et al., 1990) with the options “-p blastp - e 1e-10 -b 5 -v 5 -m 8” and ran mcscanx with the parameters “-s 5 -b 2,” requiring synteny blocks to contain at least five consecutive genes and to have a gap of no more than 25 genes. mcscanx results were visualized with synvisio (https://synvisio.github.io/#/).








[HPC@LSU invites you to attend our weekly training scheduled every Wednesdays, except university holidays.](http://www.hpc.lsu.edu/training/tutorials.php)


My Indian friend Prabba encountered a problem regarding windows size and step size. How to automated it the process?


I have got an answer her in biostar


and more resource in bedtools page


In summary to do it easy, first make the file A chromosome by chromosome with your wanted windows size


the prepare the file B like this


then run bedtools to get the results

Solution 2 is to use samtools faidx to index, sort it then use this command line


```bash

# 01. make the index file for the genome file

samtools faidx genome.fa

# 02. Explore fasta index file
less genome.fa.fai

# 03. Make 1Mb sliding windows (step 200kb)
bedtools makewindows \
-g <( grep '^X' genome.fa.fai ) \
-w 1000000 \
-s 200000 \
-i winnum \
> windows_1mb.bed

# 04. Obtain densities of genes within individual windows
bedtools coverage \
-a <( sortBed -i Ensembl.NCBIM37.67.bed ) \
-b windows_1mb.bed \
> gdens_windows_1mb.tab


```

This solution from [here](https://ngs-course.readthedocs.io/en/praha-january-2016/07-gtools.html)


The simplest way as I mentionned above is like this


```bash

$ cat A.bed
chr1  0   100
chr1  100 200
chr2  0   100

$ cat B.bed
chr1  10  20
chr1  20  30
chr1  30  40
chr1  100 200

$ bedtools coverage -a A.bed -b B.bed
chr1  0   100  3  30  100 0.3000000
chr1  100 200  1  100 100 1.0000000
chr2  0   100  0  0   100 0.0000000


```




the file A is the chromosome already set with a window size (here 100 bp). Then prepare based of the gf file the file B of your gene of interest by providing chromosome ID start and end. Then use the command


```bash

bedtools coverage -a A.bed -b B.bed

```


The third column give the gene density.  This data can be used later to make a plot in R.


A very nice explanation of the concept of window size and step size is this [biostar page](https://www.biostars.org/p/418244/)




I found SciLifelab old workshop in 2018 for [annotation](https://github.com/SciLifeLab/courses/tree/gh-pages/annotation/2018/practical_session). [RNASEQ](https://github.com/SciLifeLab/courses/tree/gh-pages/rnaseq/labs) also. [GSA](https://github.com/SciLifeLab/courses/blob/gh-pages/rnaseq/labs/GSA_tutorial.md)



Day 2 of EI Long read RNA synposium

I saw their [github page](https://github.com/EI-CoreBioinformatics) . Awesome discovery about genome annotation. The genome annotation workshop link is [here](https://github.com/EI-CoreBioinformatics/annotation-workshop-2021). And I discovered a new annotation tool called Robust Eukaryotic Annotation Toolki (REAT) [HERE](https://github.com/EI-CoreBioinformatics/reat)





# How to extract all N positions from a genome (Fasta file)?

## Brainstorming

 I will use a toy fasta for illustration. Here is it:


```bash

$ cat toy.fasta


>chr01
AAANNNNNGCT
>chr02
AAAANNNNNGCT


```

My toy has just two chromosmes. The first one has N from 4 to 8 and the second one from 5 to 9 ( **1-based coordinate**)


My initial task for my whole genome project is to know the position of those gaps in each sequence. It is important for me to know the gaps' positions precisely in my Hi-C based assembly that I have received from [Dovetails genomics](https://dovetailgenomics.com/) company.

Knowing those gaps will help me to design appropriate markers, test them using PCR method and fill the gaps accordingly. So how can I get the exact position of my gaps?


## Solution


After googling, I found on biostar this [solution](https://www.biostars.org/p/152592/). Prakki Rama suggests this [approach](https://www.biostars.org/p/133742/#377084) as a one line perl-based code.

That is awesome! I love one line solution! Let's test it!

Using the toy data, I proceeded like this:



```bash

perl -ne 'chomp;if( />(.*)/){$head = $1; $i=0; next};@a=split("",$_); foreach(@a){$i++; if($_ eq "N" && $s ==0 ){$z=$i-1; print "$head\t$z"; $s =1}elsif($s==1 && $_ ne "N"){$j=$i-1;print "\t$j\n";$s=0}}' toy.fasta


```

Here is the result:

```bash

chr01   3       8
chr02   4       9

```

**The first number indicated a zero-based coordinate while the second number indicates a 1-based coordinate.**

So in practical case, using a 1-based coordinate the start position will be 3 +1 bp = 4  for the chr01 and 4 + 1 bp = 5 for chr02.


My plan is to use samtools for cutting my sequence. Samttols uses 1-base coordinate. So my appropriate result will be:


```bash

chr01   4       8
chr02   5       9

```

That's it! Big thank to Prakki Rama.


## Bonus

To get the full N sequence, it is possible to get it by doing:


```bash

$ bedtools getfasta -fi toy.fasta -b toy.bed


```


I got this result



```bash

>chr01:3-8
NNNNN
>chr02:4-9
NNNNN


```

Note: Zero-based and one-based coordinate need to be considered for downstream analysis. 
To get more info about zero and one-based coordinate, please refer to bedtools page [here](https://bedtools.readthedocs.io/en/latest/content/overview.html). 

 - *BED starts are zero-based and BED ends are one-based*.
 - **GFF starts and ends are one-based.**
 - **VCF coordinates are one-based**

























## ELIXIR FONDUE Datathon | Day 1 Tuesday June 15th 2021

Sebastian Beier Plant Leader for Elixir plant scientific community | Contact le pour une aide 

MIAPPE 

https://www.denbi.de/training-courses-2021


Valerie Murigneux a publie un  truc encore juste avec hifi... Tres drole...


[Le papier](https://gigabytejournal.com/articles/24) | [Le code pour IPA assembly](https://www.protocols.io/view/ipa-assembly-for-pacbio-hifi-reads-buxvnxn6)


![img](https://github.com/Yedomon/bioinformatics101/blob/main/Capture.PNG)


Rgarde comment Eche a fait pour son PacBio



![Poster](https://github.com/Yedomon/bioinformatics101/blob/main/Eche.png) | SMRT Leiden 2021 | Eche








Theory of genomics selection | [Youtube Channel of Paulo izquierdo romero](https://www.youtube.com/channel/UCgbVs_9uQCEEtuU008nvgGA/videos) | [Hand on session](https://youtu.be/cdTfzjRH-Wo) | [Diagram de genomics breeding](https://youtu.be/y3Uy-x86odE?t=397)


Today 11 June 2021, Just found by chance a nice youtube [video](https://youtu.be/p4NyOGX7CyM) on SNP_calling by AG2PI workshop. And I catched at this [time](https://youtu.be/p4NyOGX7CyM?t=1980), a github link for genome assembly annotation [SNP calling](https://github.com/bcbc-group/SNP_Calling/blob/master/snpcalling_pipeline_gatk_script_2018.sh) and so on. BTI Computational Biology Center made it. Available [here](https://github.com/bcbc-group). I was amazed by the annotation section of [2020](https://github.com/bcbc-group/Botany2020NMGWorkshop/tree/master/annotation) and [2021](https://github.com/bcbc-group/NMGWorkshop2021/tree/main/5.Annotation/scripts)

Before setting a genome project, see what [to do](https://github.com/bcbc-group/NMGWorkshop2021/blob/main/3.ExperimentalDesign/Experimental_design.pdf)

All codes are available on my [Zenodo](https://zenodo.org/record/4926203#.YMNUVr7itPY)

DomainViz: intuitive visualization of consensus domain distributions across groups of proteins  | [Paper](https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkab391/6281474?login=true) | [Site](https://uhrigprotools.biology.ualberta.ca/domainviz)




[QTL mapping analysis tutorial: By Jorge Valenzuela Antelo](https://youtu.be/4RF7LRggY44)








[genomics lite A new programme of live webinars for upper secondary science students](https://publicengagement.wellcomeconnectingscience.org/genomics-lite)


[Wellcome Connecting Science Public Engagement 	Youtube Channel](https://www.youtube.com/channel/UCUzZKO3uzq8PQBeJhLLDxQg/videos)




Just released today 08th June 2021. An ITol tool alternative in R

- [Paper](https://academic.oup.com/mbe/advance-article/doi/10.1093/molbev/msab166/6294410) | [Usage](https://bioconductor.org/packages/devel/bioc/vignettes/ggtreeExtra/inst/doc/ggtreeExtra.html) | [Github](https://github.com/YuLab-SMU/ggtreeExtra/)












2021 June 02 | [Gentle introduction on PacBio Hifi](https://youtu.be/1aqnLQ-nwYk)

MCScan code


I will use diamond for blastp and synvisio as visualization tool.

- Very important point 1: eliminate * symbole from the protein file


- Vey important point 2: Arrange the GGF file following the example from the github page in data


```
at1	AT1G01010	3631	5899

```

For A.majus use **mRNA** 

```
am8	Am08g37680.T01	3631	5899


```



fOR SESAMUM INDICUM use **CDS**

```
si1 SIN_1021768

```
For Lycopersicum use **mRNA**


```
sl3 Sol 
```




- Very important point 3: Rename the chromosome id by doinf AT1 for Arabidopsis thaliana chromosome 1


- Very important point 4: Performe the blastp run for every single pairing like if you have twop genomes A and B, do the blastp for A vs A, B vs B, A vs B and B vs A

- Very important point 5: for the GFF file eliminate with excel the duplicated genes 
- Very important point 6: Concatenate all blastp file as weell as gff files 
- After running use the output file for synvisio online. It is much more faster


Note: In some protein file, there is "*" . Diamond tools is sensitive to those star symboles. So It is important to remove them by doing:


```bash

cat final_perilla4x.protein_rename.fasta | sed 's/[*]//g ' >  final_perilla4x.protein_rename_cleaned.fasta


```



After getting the result of blast, you may need to convert the csv format into a tabulate format before running mcscan. So do:


```bash

sed 's|,|\t|g' pc_pf.blast.csv > pc_pf.blast

```




# Step 1: blastp  of pc versus pc

```python

source activate diamond_env

diamond makedb --in perilla_v1.0_protein_without_point.fasta  -p 64 -d perilla_v1.0_protein_without_point

diamond blastp -d perilla_v1.0_protein_without_point -q perilla_v1.0_protein_without_point.fasta -p 64 --evalue 0.00001 --out pc_vs_pc_diamond.cleaned.csv --outfmt 6 &> log.run.diamond.cleaned.pc_pc &

```

# Step 2: blastp of pc versus at


```python

diamond makedb --in Athaliana_447_Araport11.protein.cleaned.fa  -p 64 -d Athaliana_447_Araport11.protein.cleaned


diamond blastp -d Athaliana_447_Araport11.protein.cleaned -q perilla_v1.0_protein_without_point.fasta -p 64 --evalue 0.00001 --out pc_vs_ara_diamond.cleaned.csv --outfmt 6 &> log.run.diamond.cleaned.pc_ara &

```


# Step 3: blastp of at versus pc

```python

diamond makedb --in perilla_v1.0_protein_without_point.fasta  -p 64 -d perilla_v1.0_protein_without_point

diamond blastp -d perilla_v1.0_protein_without_point -q Athaliana_447_Araport11.protein.cleaned.fa -p 64 --evalue 0.00001 --out ara_vs_pc_diamond.cleaned.csv --outfmt 6 &> log.run.diamond.cleaned.ara_pc &

```

# step 4:blastp at versus at

```python
diamond makedb --in Athaliana_447_Araport11.protein.cleaned.fa  -p 64 -d Athaliana_447_Araport11.protein.cleaned


diamond blastp -d Athaliana_447_Araport11.protein.cleaned -q Athaliana_447_Araport11.protein.cleaned.fa -p 64 --evalue 0.00001 --out ara_vs_ara_diamond.cleaned.csv --outfmt 6 &> log.run.diamond.cleaned.ara_ara &

```



# Step 5: Concatenate both

```python

cat pc_vs_pc_diamond.cleaned.csv pc_vs_ara_diamond.cleaned.csv ara_vs_pc_diamond.cleaned.csv ara_vs_ara_diamond.cleaned.csv > pc_at.blast

```























Just see on twitter new tool for read mapping using long-reads | [Vulcan](https://gitlab.com/treangenlab/vulcan) | Use [vulcan](https://www.biorxiv.org/content/10.1101/2021.05.29.446291v1?s=03) for mapping and [sniffles](https://github.com/fritzsedlazeck/Sniffles/wiki) for SV calling |

Un bon [tutorial](https://ressources.france-bioinformatique.fr/sites/default/files/5%20-%20Variants%20Structuraux.pdf) sour les SV.





```

conda install -c bioconda vulcan

conda install -c bioconda sniffles


./vulcan -r ./test/GCF_000146045.2_R64_genomic.fna -i ./test/test_reads.fa -w ./test/ -o vulcan


./sniffles -m mapped.sort.bam -v output.vcf

```


Awesome disovring today 30 May 2021 | [Video](https://youtu.be/P18iAOj9LEk) | [bioarchive](https://www.biorxiv.org/content/10.1101/2021.03.12.435103v1.full) | [code avaliablity](https://github.com/UMNKimballLab/NWRGenomeAssembly_v1.0)


Each figure est bien expliquee avec les codes a l'appuie. Jai sauve les codes ici dans le github. Awesome.






TASSEL workshop  [video1](https://youtu.be/BY_6qSIW4Rk) | [video2](https://youtu.be/hiReCEINEVs) | [google group](https://groups.google.com/g/tassel) | [github](https://github.com/SushanRu/TASSEL-workshop) | [
University of Minnesota Plant Breeding Center](https://www.youtube.com/channel/UCl1JgjWhfTqG7DqRl6j_zlA)










From this [Assembling vertebrate genomes](https://www.nature.com/articles/s41576-021-00379-z) Arang Rhie suggests 

> The highest-quality Anna’s hummingbird genome was achieved by generating haplotype-separated contigs using Pacific Biosciences continuous long reads, followed by scaffolding with 10× Genomics linked reads, Bionano optical maps and Phase Genomics Hi-C; gap filling, base call polishing and manual curation completed the workflow.



So haplotipe resolves contigs assembly > Scaffolding with 10x genomics > Bionano optical map > Hi-C > Gap filling > Manual correction







Watching SMRT Leiden 2021 | Leaf to go presentation By Luca Ermini


[LongQC tool for quality control](https://www.g3journal.org/content/10/4/1193#:~:text=LongQC%20is%20a%20computationally%20efficient,introduced%20at%20the%20sequencing%20stage.)  

Utilise cela pour ton article


[svpack]() | new hifi asssembler called [LJA](https://www.biorxiv.org/content/10.1101/2020.12.10.420448v1.full)



# Make a gene density file using RIdeogram R package


```r

require(RIdeogram)
library(RIdeogram)

Mihong_karyotype <- read.table("Mihong_Chr.txt", sep = "", header = T, stringsAsFactors = F)
Mihong_karyotype


Mihong_gene_density-2 <- GFFex(input = "Mihong_GFF.gff", karyotype = "Mihong_Chr.txt", feature = "gene", window = 100000)
Mihong_gene_density

ideogram(karyotype = Mihong_karyotype, overlaid = Mihong_gene_density)
convertSVG("chromosome.svg", device = "png")

```


Note:

The karyotype should be formatted as follow:

- Save in txt file in excel. csv doesn't work!!!!!!!
- for the GFF file ........










[A pseudomolecule-scale genome assembly of the liverwort Marchantia polymorpha](https://onlinelibrary.wiley.com/doi/full/10.1111/tpj.14602)

Tres bon boulot surout pour la detection des centromeres 

Seydina Issa. Diop,



Oliver Subotic,Alejandro Giraldo-Fonseca,Manuel Waller,Alexander Kirbis,Anna Neubauer,Giacomo Potente,Rachel Murray-Watson,Filip Boskovic,Zoe Bont,Zsofia Hock,Adam C. Payton,Daniël Duijsings,Walter Pirovano,Elena Conti,Ueli Grossniklaus,Stuart F. McDaniel,Péter Szövényi


[CuteSV github](https://github.com/tjiangHIT/cuteSV) | [Paper](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02107-y)

Diapo 1

![img](https://github.com/Yedomon/bioinformatics101/blob/main/cuteSV.PNG)

Diapo 2

![img](https://github.com/Yedomon/bioinformatics101/blob/main/cuteSV2.PNG)


Diapo 3

![img](https://github.com/Yedomon/bioinformatics101/blob/main/cuteSV3.PNG)


[Genome Informatics Facility](https://github.com/ISUgenomics)



[Performance of Mapping Approaches for Whole-Genome Bisulfite Sequencing Data in Crop Plants](https://www.frontiersin.org/articles/10.3389/fpls.2020.00176/full?report=reader#h3)

![img](https://www.frontiersin.org/files/Articles/504419/fpls-11-00176-HTML/image_m/fpls-11-00176-g002.jpg)


[Code availability](https://github.com/grehl/benchWGBSmap)



[A perl script to visualize intergated genetic map(s) and synteny map(s) at chromosome level](https://github.com/XuepengSun/MapViewer)

```perl
version: v0.5 (03/09/2018)
Useage:
	
	perl MapViewer.pl [options]
	
options:
	[input]
		-syn	synteny file name [if set two files, "map" is not allowed; <=2]
		-map	map file name [multiple files can be specified with comma]
		-gap	gap file, gaps between scaffods in the pseudochromosome
		-help	this help information
		
	[display]
			parameters for display can be adjusted within the script
			
	[file format]
		synteny file: 
			<chr_id>\t<chr_length>\t<chr_start>\t<chr_end>\t<ref_chr_id>\t<ref_chr_length>\t<ref_chr_start>\t<ref_chr_end>
		
		map file:
			<chr_id>\t<chr_length>\t<chr_pos>\t<LG_id>\t<LG_distance>\t<LG_position>
			
		gap file:
			<chr_id>\t<gap start position>\t<gap length>
			

```

![img](https://github.com/Yedomon/bioinformatics101/blob/main/Chr1.png)


- #### [ISMB 2020 Training on long read transcriptome](https://zenodo.org/record/4732317#.YI1yLu3iuUk)


- #### Just found today this south american bioinfomatics society [github](https://github.com/eead-csic-compbio?tab=repositories)



- #### Variant calling


Amaizing tutorial with GATK from ScilifeLab [NGS workflow](https://scilifelab.github.io/courses/ngsintro/1805/labs/NGS_workflow)

An other one found [here](https://github.com/ReiGao/GWSBE/blob/master/2.bwa.sh)

An other one [here](http://costalab.org/wp-content/uploads/2017/05/lecture_3_ngs.pdf)

Great [PPT1](https://warwick.ac.uk/fac/sci/statistics/staff/academic-research/nichols/presentations/ohbm2014/imggen/Nho-ImgGen-WGSeqPractical.pdf0)

Great [PPT2](https://scilifelab.github.io/courses/ngsintro/1509/slides/NGS_AJ.pdf)

[Pipeline explained](https://learn.gencore.bio.nyu.edu/variant-calling/pre-processing/)

Amaizing tools for Hi-C visualisation and other genomics data | available on conda

- #### 2021 | [CoolBox: A flexible toolkit for visual analysis of genomics data](https://www.biorxiv.org/content/10.1101/2021.04.15.439923v1)

- #### 2020 | [Cooler: scalable storage for Hi-C data and other genomically labeled arrays](https://academic.oup.com/bioinformatics/article/36/1/311/5530598)


BREAKING NEWS Today 18 april 2021| Heng Li [tweet](https://twitter.com/lh3lh3/status/1383515558306451465)


> Hifiasm v0.15 released with much improved Hi-C mode especially for non-human species. Also for HiFi-only data, it now outputs two partially phased complete assemblies in addition to primary+alternate contigs. 


So it is possible to get the phased diploid assembly. Then perform a scaffolding later.... NB: Hifiasm is not yet a scaffolder...


Great! 


A good idea is to use hifi data + Hifiasm + hi-c data + SALSA or 3DNA to get quite good diploid accurate chromosome scale data. 







Long-read-tools.org: an interactive catalogue of analysis methods for long-read sequencing data [WEBSITE](https://long-read-tools.org/)



[Samtools markdup for duplicate removal or Picard?](https://www.researchgate.net/post/Samtools_markdup_for_duplicate_removal_or_Picard)


question: transpose-multiple-rows-into-a-single-column


Answer [here](https://www.linuxquestions.org/questions/linux-newbie-8/transpose-multiple-rows-into-a-single-column-873623/)

For space tab delimitade
```pyton 

awk 'RT' RS=" " test.input

```
For comma separeted file

```python

awk 'RT' RS="," test.comma.input > out

```


- #### [snp_CAPS](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4785002/pdf/66_244.pdf) | [Lee 1](http://www.koreabreedjournal.org/journal/view.html?doi=10.9787/KJBS.2014.46.2.116) | [Lee 2](http://www.plantbreedbio.org/journal/view.html?volume=8&number=3&spage=293&year=2020)



- #### [Plant Breeding and Genomics Learning Lessons](https://plant-breeding-genomics.extension.org/plant-breeding-and-genomics-learning-lessons/)

- #### [SNP-Based Genetic Linkage Maps for Potato](https://plant-breeding-genomics.extension.org/snp-based-genetic-linkage-maps-for-potato/)

- #### Integrating Molecular Biology and Bioinformatics Education| [Paper](https://www.degruyter.com/document/doi/10.1515/jib-2019-0005/html) | [AppliedGenomeResearch](https://github.com/bpucker/AppliedGenomeResearch)


- #### [Interactive introduction to statistics and probabilities](https://seeing-theory.brown.edu/#firstPage)



- #### [Speciation & Population Genomics: a how-to-guide](https://speciationgenomics.github.io/)



Blog

- #### [sr-c blog](https://sr-c.github.io/)

- #### [zhugueng blog](https://xuzhougeng.blog.csdn.net/)

- #### [jianshu blog](https://www.jianshu.com/u/740f4b0f11e9)

- #### [ZHUBLOG](https://zhuanlan.zhihu.com/)




Paper


[High-density genetic map using whole-genome resequencing for fine mapping and candidate gene discovery for disease resistance in peanut](https://onlinelibrary.wiley.com/doi/epdf/10.1111/pbi.12930)


Flash


- #### [genome annotation v1](https://blog.csdn.net/u012110870/article/details/82500684) key words `基因组注释植物博客`


- #### [rnaseq nextflow](https://sr-c.github.io/2020/05/25/RNA-seq-DiffExp-analysis/)

- #### [jvciMCscan python](https://sr-c.github.io/2019/01/11/jcvi-MCscan/) | [original tutorial](https://github.com/tanghaibao/jcvi/wiki/MCscan-%28Python-version%29) | [additionnal tuto](https://www.jianshu.com/p/39448b970287)

- #### [MCMC V1](https://www.jianshu.com/p/b12e058c6597) [MCMC V2](http://www.chenlianfu.com/?p=2974)  [MCMC Tree](https://www.jianshu.com/p/b12e058c6597)

- #### [paml parat ka ks codeml](https://blog.csdn.net/weixin_42376118/article/details/112065784) | [ka ks v2](http://blog.sciencenet.cn/blog-3433349-1241328.html)


- #### [pipeline for comparative genomics and evolutionnary](https://blog.csdn.net/qq_36608036/article/details/109466468?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.baidujs&dist_request_id=1328626.667.16153013339855743&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.baidujs)


- #### [rna SEQ](https://zhuanlan.zhihu.com/p/61847802)

- #### [CAFE tutorial](https://www.jianshu.com/p/146093c91e2b) | [original](https://iu.app.box.com/v/cafetutorial-pdf)
- #### [wgdi](https://xuzhougeng.blog.csdn.net/article/details/114013801)
- #### [HiC pro singularity](https://xuzhougeng.blog.csdn.net/article/details/109892648)

- How to use HPC job and so on [msu edu high coumputer plateform](https://wiki.hpcc.msu.edu/) 
- Bioinfo small [tuto](https://wiki.hpcc.msu.edu/display/ITH/Bioinformatics)
- Example [GTAK4](https://wiki.hpcc.msu.edu/display/ITH/GATK4) 
- install repeatmasker with a modified conda based [strategy](https://xuzhougeng.blog.csdn.net/article/details/102804531)
- [Installing maker with conda](https://wiki.hpcc.msu.edu/display/ITH/Installing+maker+using+conda)
- [CAFE](https://xuzhougeng.blog.csdn.net/article/details/102804514)
- #### ggplot2 [example1](https://www.jianshu.com/p/68aa08ef9f61)
- #### [mercury](https://www.jianshu.com/p/61fefb9a9c5f)
- #### [Phylogenomic_Tutorial || Bayesian Phylogenetic Inference](https://www.jianshu.com/p/7983618c51d4)
- #### [Phylogenomic_Tutorial || ML_Tree inference](https://www.jianshu.com/p/282a94b50418)



